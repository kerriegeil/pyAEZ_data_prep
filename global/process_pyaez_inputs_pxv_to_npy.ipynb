{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a86a5cb9-0a43-42a3-bf83-5d7517641a46",
   "metadata": {},
   "source": [
    "Author: Kerrie Geil, Mississippi State University, Geosystems Research Institute\n",
    "Date: March 2024\n",
    "Description: this script takes Gunther's pxv files and calculates the globally gridded daily variables needed as input to PyAEZ. Variables are written to npy stacks for each year of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d9951-bdbb-4db3-83cf-37504833177a",
   "metadata": {},
   "source": [
    "### Workflow overview\n",
    "\n",
    "For each year in the range of years specified, do the following:\n",
    "\n",
    "- read global grid (ALOS mask)\n",
    "- put monthly pxv data onto global grid\n",
    "- put daily deviation pxv data onto global grid\n",
    "- create daily data \n",
    "    - calculate daily precip, srad, tmax, tmin, vapr, 10m wind \n",
    "    - interpolate to 2m wind and change units for wind and srad\n",
    "    - write as npy stacks (precip, srad, tmax, tmin, vapr, 2m wind) \n",
    "- compute daily relative humidity and write as npy stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81dc14e-8237-494d-91ce-3e84e29ec282",
   "metadata": {},
   "source": [
    "### Processing time\n",
    "\n",
    " approximately 8-10 minutes using a full node (40 tasks) to process each year (includes all 6 daily pyaez variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58e45d-1604-4e97-8178-69827727264d",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "You cannot modify the code below to process srad or wind on its own. You must process these variables with precip. \n",
    "\n",
    "This is because valid srad and wind values in the pxv files span over the fill value used (-9999). So, we need to create a mask from a different variable in order to correctly identify which srad and wind grids should be scaled vs which are masked grids. I've chosen precip to create the additional mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f291f224-01f0-4f4e-bafd-8d2a9d1dd8ff",
   "metadata": {},
   "source": [
    "### Python environment information\n",
    "\n",
    "This notebook must be run in an environment where the following packages are installed:\n",
    "\n",
    "xarray, rioxarray, numpy, pandas, dask\n",
    "\n",
    "and matplotlib is optional if you want to uncomment the plotting portions of the loop\n",
    "\n",
    "It is suggested that the user builds a conda environment with these packages on HPC Orion using the system module miniconda. The user also needs to create a jupyter kernel from this conda environment before launching the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76be7b1a-ab66-4fab-8b9f-6cc45a11483b",
   "metadata": {},
   "source": [
    "### How to launch this notebook\n",
    "\n",
    "Instructions for launching are provided elsewhere for security of the system. Please ask Kerrie if you need a copy of the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab623db8-1701-4fd1-a1c7-128a2eb7d63f",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73521876-ab5a-41f0-8277-33c6a3f26512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from calendar import isleap\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import dask\n",
    "import rioxarray as rio\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import time as timer\n",
    "import logging\n",
    "logging.captureWarnings(True)\n",
    "\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0541f226-e688-4da9-a65a-6e227f9614f4",
   "metadata": {},
   "source": [
    "### Things we need to know up front from Gunther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3859e2f6-997e-46bc-b03a-32b8af1237ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) shape of the ALOS mask. This is the global grid ny and nx including Antarctia. \n",
    "ny_global=2160\n",
    "nx_global=4320\n",
    "\n",
    "# 2) shape of the global grid ny and nx excluding Antarctica. (ny corresponds to IRmax in Gunther's fortran program)\n",
    "ny=1800\n",
    "nx=nx_global\n",
    "\n",
    "# 3) data type of what's inside the daily deviation and monthly pxv files \n",
    "# fortran 2 byte int = python np.int16\n",
    "# fortran 4 byte float = np.float32\n",
    "dtype_d=np.int16   # 2 byte integers for daily data\n",
    "dtype_m=np.float32 # 4 byte floats for monthly data\n",
    "\n",
    "# 4) fill value used in the daily deviation and monthly pxv files\n",
    "fillval_d=-9999   # daily\n",
    "fillval_m=-9999.0 # monthly\n",
    "\n",
    "# 5) total number of data points (number of lines) in the pxv files (this includes points set to the fillval) \n",
    "npts=2295358\n",
    "\n",
    "# 6) total number of data points in the pxv files with valid data values\n",
    "# this is so we can identify which points should be masked and which should be scaled \n",
    "# when the data range spans over the fillval which is a problem for srad and wind\n",
    "npts_valid_d=2287408  # daily files\n",
    "npts_valid_m=2268708  # monthly files\n",
    "\n",
    "# 7) scale factors for putting the data in the pxv files into the units in the table below\n",
    "\n",
    "# Variable\tMonthly data\tDaily deviations/distr.\tScale factor\n",
    "# Precip\t     mm/day\t         %_of_month×100\t       0.0001\n",
    "# Srad\t       J/m2/day\t            kJ/m2/day\t        1000.\n",
    "# Tmax\t         °C\t                 °C×100\t            0.01\n",
    "# Tmin\t         °C\t                 °C×100\t            0.01\n",
    "# Vapr\t         hPa\t                Pa\t            0.01\n",
    "# Wind\t        m/sec\t              mm/sec\t        0.001\n",
    "\n",
    "# in alphabetical order by variable name \n",
    "scale_factors=[0.0001,1000.,0.01,0.01,0.01,0.001]\n",
    "\n",
    "# 8) how many decimal places the daily output variables should have\n",
    "output_trunc=[4,2,3,3,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734edd7f-365f-4780-ac80-3a07dc6a41f6",
   "metadata": {},
   "source": [
    "### other constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da41d15-99ee-4198-a47b-0cb251974dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fao_basedir='/change/this/to/the/shared/datasets/direcory/' # the group's shared datasets dir\n",
    "\n",
    "# pxv things\n",
    "pxv_basedir=fao_basedir+'gaez_v5/clim/AgERA5/Hist/' # this is where Gunther has the pxv files\n",
    "dataset='AgERA5'\n",
    "experiment='Hist'\n",
    "pxvsuf='_5m.pxv'\n",
    "connector='_'\n",
    "dailytag='365'\n",
    "sep='/'\n",
    "pxvdirnames=['prec','srad','tmax','tmin','vapr','wind']\n",
    "varnames=['Precip','Srad','Tmax-2m','Tmin-2m','Vapr','Wind-10m']\n",
    "\n",
    "# raster things\n",
    "maskfile=fao_basedir+'gaez_v5/land/ALOSmask5m_fill.rst'\n",
    "ydimname='y'\n",
    "xdimname='x'\n",
    "\n",
    "# output things\n",
    "out_basedir=fao_basedir+'pyaez/inputs/global/daily365_npy/'\n",
    "\n",
    "# parallel computing things\n",
    "xrchunks={'time':-1,'y':-1,'x':54} # 80 chunks, xr format\n",
    "dachunks=(-1,54,-1) # the same 80 chunks, da format\n",
    "\n",
    "# months to process\n",
    "nmonths=12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15abcf94-1c83-4b8d-b107-cc59b5fa72e7",
   "metadata": {},
   "source": [
    "### user to input which years to process here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889a189-39f3-48d8-bf1c-db3cd7ce25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "years=np.arange(2021,2024) # the end year is not inclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8cdee0-f6a4-453f-aaad-9f5a965e2cbb",
   "metadata": {},
   "source": [
    "# Begin Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691fb21c-bb6f-4421-9bd2-f61e839427bf",
   "metadata": {},
   "source": [
    "### First, get the ALOS mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4342e2-7728-4135-bacb-8907a00abd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the mask from rst into an array of 2 dims (y,x)\n",
    "### and check that it has the expected number of data points\n",
    "\n",
    "# open the maskfile but don't include antarctica so mask has shape (y:1800,x:4320)\n",
    "ds=xr.open_dataset(maskfile,engine='rasterio').isel(y=slice(0,ny)).squeeze()\n",
    "del ds.coords['band']\n",
    "\n",
    "# clean up some metadata\n",
    "ds[xdimname]=ds[xdimname].astype(np.float32)\n",
    "ds[ydimname]=ds[ydimname].astype(np.float32)\n",
    "mask2D=ds.band_data\n",
    "\n",
    "# convert to 0 & 1 mask\n",
    "mask2D=xr.where(mask2D>0,1,0).astype('int8')\n",
    "\n",
    "mask1D=mask2D.stack(space=[ydimname,xdimname]) # collapse mask to 1D: 1800*4320 = 7776000 points\n",
    "inds_data=mask1D==1  # keep track of which points are not masked out\n",
    "\n",
    "# error checking\n",
    "npts_mask=int(mask2D.sum().data) # number of data points in mask\n",
    "print('total data points in mask file', npts_mask,'expecting',npts)\n",
    "# if this throws an error, coordinate with Gunther\n",
    "assert npts_mask==npts, f\"data points expected {npts}, data points present in mask {npts_mask}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe443d17-a6e7-4797-8b2a-cca98a4fa811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function puts data from pxv files onto a 2-dimensional global grid\n",
    "# we will call it later with dask delayed to compute in parallel\n",
    "def data_to_nd_array(i,inds,arr1D,pxv,arr2D):\n",
    "    arr1D[inds]=pxv.squeeze()  # remove singleton dim (day)\n",
    "    arr2D[:,:]=arr1D.unstack() # put 1D data onto the 2D grid\n",
    "    return arr2D.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a22692-e478-48cf-8672-58e53a3f0949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main data processing loop through years\n",
    "# inputs: the monthly and daily deviation pxv files from Gunther\n",
    "# outpus: npy stacks of daily precip, srad, tmax, tmin, 2m wind, and rhum for pyaez\n",
    "\n",
    "for yyyy in years:\n",
    "    \n",
    "    start_time=timer()\n",
    "    print('########################################################################')\n",
    "    print('################################',yyyy,'################################')\n",
    "    print('########################################################################')\n",
    "\n",
    "    for varind, var in enumerate(varnames):\n",
    "        \n",
    "        print('*****************************************')\n",
    "        print('*************** Processing',varnames[varind],'***************')    \n",
    "        print('*****************************************')\n",
    "        \n",
    "        ######################################################################################################\n",
    "        #  STEP 1: Translate monthly means from PXV to xarray data structures on global grid    \n",
    "        ######################################################################################################\n",
    "        print('################################ STEP1: PROCESSING MONTHLY PXV ################################')\n",
    "        pxvfile_m=pxv_basedir+pxvdirnames[varind]+sep+varnames[varind]+connector+dataset+connector+experiment+connector+str(yyyy)+pxvsuf\n",
    "        filename=pxvfile_m.split(sep)[-1]\n",
    "\n",
    "        # read file to 1D array\n",
    "        # monthly files have more data in them than we need so we subset the read with count=nmonths*npts\n",
    "        with open(pxvfile_m,'rb') as f:\n",
    "            array1D_m=np.fromfile(f,dtype=dtype_m,count=nmonths*npts)\n",
    "\n",
    "        # limit precision here?\n",
    "\n",
    "        # error checking\n",
    "        nvals=array1D_m.shape[0]             # total number of data values from the file\n",
    "        npts_flt=nvals/nmonths               # number of grid points in the file, float format\n",
    "        npts_int=int(nvals/nmonths)          # convert to integer\n",
    "        # check number of grids found in flt and int are equivalent, if not, the file was read incorrectly\n",
    "        assert npts_flt*10==float(npts_int*10), f\"reading pxv file {filename} with incorrect number of days: {ndays}\"\n",
    "        # check number of grids found in monthly pxv data file is the number expected, if not, coordinate with Gunther\n",
    "        assert npts_int==npts, f\"pxv file {filename} has {npts_int} total data points, expecting {npts} total data points\"\n",
    "        print(f\"total data points from monthly pxv file {npts_int}, expecting {npts}\")\n",
    "\n",
    "        # reshape the array to 2 dimensions (npoints,ndays)\n",
    "        array2D_m=array1D_m.reshape(npts,nmonths) # reshape\n",
    "\n",
    "        # find out if data value range spans across the fillvalue\n",
    "        # if it does, we'll need to apply an extra mask later\n",
    "        flag_m=True if array2D_m.min() < fillval_m else False\n",
    "\n",
    "        # check data values\n",
    "        print('data min/max values',array2D_m.min(),array2D_m.max())\n",
    "        print('apply extra mask?',flag_m)\n",
    "\n",
    "        # set up for putting data on full grid\n",
    "        empty1D_m=mask1D.copy().astype(dtype_m)            # placeholder array for 1D space \n",
    "        empty1D_m.rio.write_nodata(fillval_m,inplace=True) # set the fill value attribute\n",
    "        empty1D_m[:]=fillval_m                             # fill array with fill value\n",
    "\n",
    "        empty2D_m=mask2D.copy().astype(dtype_m)            # placeholder array for 2D grid \n",
    "        empty2D_m.rio.write_nodata(fillval_m,inplace=True) # set the fill value attribute\n",
    "        empty2D_m[:,:]=fillval_m                           # fill array with fill value\n",
    "\n",
    "        # put the monthly pxv data onto a global grid using dask parallel computing \n",
    "        # first convert data to a chunked dask array object, 1 day per chunk \n",
    "        # and save array chunks to a list of delayed dask objects\n",
    "        pxv_delay=da.from_array(array2D_m,chunks=(-1,1)).to_delayed().ravel() \n",
    "\n",
    "        # using the function we defined earlier (data_to_nd_array),\n",
    "        # build a list a computational tasks to be executed in parallel\n",
    "        task_list=[dask.delayed(data_to_nd_array)(imonth,inds_data,empty1D_m.copy(),pxvdata,empty2D_m.copy())\\\n",
    "                   for imonth,pxvdata in enumerate(pxv_delay)] \n",
    "        # double check there is 1 task per day of data\n",
    "        assert len(task_list)==nmonths, f'{len(task_list)} tasks in list, should be {nmonths}' \n",
    "\n",
    "        # execute all computations\n",
    "        print('putting 1D data on a 2D grid...')\n",
    "        result_chunks_m=dask.compute(*task_list)\n",
    "\n",
    "        # concatenate the resulting daily data chunks along a new time dimension\n",
    "        print('concatenating...')\n",
    "        data3D_m=xr.concat(result_chunks_m,dim='time')\n",
    "\n",
    "        # replace fillval with nan\n",
    "        print('adding nans...')\n",
    "        data3D_m=xr.where(data3D_m==fillval_m,np.nan,data3D_m)\n",
    "\n",
    "        # check we have correct number of non-missing data points\n",
    "        data_mask_m=xr.where(np.isnan(data3D_m.data),0,1)         # 0 where nan, 1 where not nan for all times\n",
    "        ngrids_data_m=int(data_mask_m.sum()/data_mask_m.shape[0]) # divide by time to get data points per time step\n",
    "        print('total number of non-missing data points',ngrids_data_m,'expecting',npts_valid_m)\n",
    "        assert ngrids_data_m==npts_valid_m, f'data mask creation issue. found {ngrids_data_m} valid data points (non missing), expecting {npts_valid_m}'    \n",
    "\n",
    "        # visual check January\n",
    "        # figure=plt.figure(figsize=(6,4))\n",
    "        # data3D_m.isel(time=0).plot()\n",
    "        # plt.title(varnames[varind]+' data from monthly mean pxv, Jan '+str(yyyy))\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        del array1D_m,pxvfile_m,filename,nvals,npts_flt,npts_int,array2D_m,flag_m,empty1D_m,empty2D_m,pxv_delay,task_list,result_chunks_m\n",
    "        ######################################################################################################\n",
    "        # END STEP 1\n",
    "        #####################################################################################################\n",
    "\n",
    "        ######################################################################################################\n",
    "        # STEP 2: Translate daily deviations from PXV to scaled xarray data structures on global grid\n",
    "        ######################################################################################################\n",
    "        print('################################ STEP 2: PROCESSING DAILY DEV PXV ################################')\n",
    "        pxvfile=pxv_basedir+pxvdirnames[varind]+sep+varnames[varind]+dailytag+connector+dataset+connector+experiment+connector+str(yyyy)+pxvsuf\n",
    "        filename=pxvfile.split(sep)[-1]\n",
    "\n",
    "        # read entire file into 1D array\n",
    "        with open(pxvfile,'rb') as f:\n",
    "            array1D_d=np.fromfile(f,dtype=dtype_d)            \n",
    "\n",
    "        # error checking\n",
    "        nvals=array1D_d.shape[0]           # total number of data points\n",
    "        ndays=366 if isleap(yyyy) else 365 # number of days of data at each grid point\n",
    "        npts_flt=nvals/ndays               # number of grid points in the file, float format\n",
    "        npts_int=int(nvals/ndays)          # convert to integer\n",
    "        # check number of grids found in flt and int are equivalent, if not, the file was read incorrectly\n",
    "        assert npts_flt*10==float(npts_int*10), f\"reading pxv file {filename} with incorrect number of days: {ndays}\"\n",
    "        # check number of grids found in daily dev pxv data file is the number expected, if not, coordinate with Gunther\n",
    "        assert npts_int==npts, f\"pxv file {filename} has {npts_int} total data points, expecting {npts} total data points\"\n",
    "        print('total data points in daily dev pxv file',npts_int,'expecting',npts)\n",
    "\n",
    "        # reshape the array to 2 dimensions (npoints,ndays)\n",
    "        array2D_d=array1D_d.reshape(npts,ndays)\n",
    "\n",
    "        # find out if data value range spans across the fillvalue\n",
    "        # if it does, we'll need to apply an extra mask later\n",
    "        flag_d=True if array2D_d.min() < fillval_d else False\n",
    "\n",
    "        # check data values\n",
    "        print('data min/max values before scaling:',array2D_d.min(),array2D_d.max())\n",
    "        print('apply extra mask?',flag_d)\n",
    "\n",
    "        # set up for putting data on full grid\n",
    "        empty1D_d=mask1D.copy().astype(dtype_d)            # placeholder array for 1D space \n",
    "        empty1D_d.rio.write_nodata(fillval_d,inplace=True) # set the fill value attribute\n",
    "        empty1D_d[:]=fillval_d                             # fill array with fill value  \n",
    "\n",
    "        empty2D_d=mask2D.copy().astype(dtype_d)            # placeholder array for 2D grid \n",
    "        empty2D_d.rio.write_nodata(fillval_d,inplace=True) # set the fill value attribute\n",
    "        empty2D_d[:,:]=fillval_d                           # fill array with fill value  \n",
    "\n",
    "        # put the daily dev pxv data onto a global grid using dask parallel computing \n",
    "        # first convert data to a chunked dask array object, 1 day per chunk \n",
    "        # and save array chunks to a list of delayed dask objects\n",
    "        pxv_delay=da.from_array(array2D_d,chunks=(-1,1)).to_delayed().ravel() \n",
    "\n",
    "        # using the function we defined earlier (data_to_nd_array),\n",
    "        # build a list a computational tasks to be executed in parallel\n",
    "        task_list=[dask.delayed(data_to_nd_array)(iday,inds_data,empty1D_d.copy(),pxvdata,empty2D_d.copy())\\\n",
    "                   for iday,pxvdata in enumerate(pxv_delay)] \n",
    "        # double check we've got 1 task per day of data        \n",
    "        assert len(task_list)==ndays, f'{len(task_list)} tasks in list, should be {ndays}' \n",
    "\n",
    "        # execute all computations\n",
    "        print('putting 1D data on a 2D grid...')\n",
    "        result_chunks_d=dask.compute(*task_list)\n",
    "\n",
    "        # concatenate the resulting daily chunks along a new time dimension\n",
    "        print('concatenating...')\n",
    "        data3D_d=xr.concat(result_chunks_d,dim='time')\n",
    "\n",
    "        # Now scale data, change fill value to nan, and apply extra mask if necessary\n",
    "        # th extra mask is required if the valid range of the data includes the -9999 fillval\n",
    "        # which is the case for srad and wind      \n",
    "        print('changing dtype...')      \n",
    "        data3D_d=data3D_d.astype(np.float32) # force float32 for nans in output\n",
    "\n",
    "        # if valid data range includes fillval (srad, wind), do scaling then additional masking\n",
    "        if flag_d:\n",
    "            print('processing pxv variable with data values that span over the fillval')\n",
    "            print('scaling...')\n",
    "            data3D_d=data3D_d*scale_factors[varind]    \n",
    "            print('applying additional mask...')\n",
    "            data3D_d=xr.where(data_mask_d,data3D_d,np.nan)\n",
    "            # verify that the masking worked\n",
    "            valid_arr=np.where(np.isnan(data3D_d.data[14,:,:]),0,1) # pick one day to verify  \n",
    "            nvalid=valid_arr.sum()\n",
    "            print('total number of non-missing data points',nvalid,'expecting',npts_valid_d)\n",
    "            assert nvalid==npts_valid_d, f'data mask application issue. found {nvalid} valid data points (non missing), expecting {npts_valid_d}'    \n",
    "            del valid_arr,nvalid\n",
    "        # if valid data range doesn't include fillval (precip,tmin,tmax,vapr), convert fillval to nan then scale\n",
    "        else:\n",
    "            print('adding nans...')\n",
    "            data3D_d=xr.where(data3D_d==fillval_d,np.nan,data3D_d)\n",
    "            print('scaling...')\n",
    "            data3D_d=data3D_d*scale_factors[varind]\n",
    "\n",
    "            # compute the additional mask from precipitation\n",
    "            if varnames[varind] =='Precip':\n",
    "                print('computing additional mask...')\n",
    "                data_mask_d=xr.where(np.isnan(data3D_d.data),0,1)  \n",
    "                ngrids_data_d=int(data_mask_d.sum()/data_mask_d.shape[0])\n",
    "                print('total number of non-missing data points',ngrids_data_d,'expecting',npts_valid_d)\n",
    "                assert ngrids_data_d==npts_valid_d, f'data mask creation issue. found {ngrids_data_d} valid data points (non missing), expecting {npts_valid_d}' \n",
    "\n",
    "        print('data min/max values after scaling:',data3D_d.min().data,data3D_d.max().data)\n",
    "                \n",
    "        # visual check January\n",
    "        # figure=plt.figure(figsize=(6,4))\n",
    "        # data3D_d.isel(time=14).plot()\n",
    "        # plt.title(varnames[varind]+' scaled data from daily dev pxv, 15 Jan '+str(yyyy))\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        del array1D_d,pxvfile,filename,nvals,npts_flt,npts_int,array2D_d,flag_d,empty1D_d,empty2D_d,pxv_delay,task_list,result_chunks_d\n",
    "        ######################################################################################################\n",
    "        # END STEP 2\n",
    "        ######################################################################################################\n",
    "\n",
    "        ######################################################################################################\n",
    "        # STEP 3: Create daily data from monthly means and daily deviations\n",
    "        ######################################################################################################\n",
    "        print('################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################')\n",
    "        \n",
    "        # create metadata labels for the time dimension\n",
    "        if (yyyy>=1980) & (yyyy<=2024):\n",
    "            time_m=pd.date_range(str(yyyy)+'-01-01',str(yyyy)+'-12-31',freq='MS')\n",
    "            time_d=pd.date_range(str(yyyy)+'-01-01',str(yyyy)+'-12-31',freq='D')\n",
    "        else:\n",
    "            time_m=pd.date_range('1900-01-01','1900-12-31',freq='MS')  \n",
    "            time_d=pd.date_range('1900-01-01','1900-12-31',freq='D') \n",
    "\n",
    "        # assign the time labels to the data arrays\n",
    "        data3D_m=data3D_m.assign_coords(time=(\"time\",time_m))\n",
    "        data3D_d=data3D_d.assign_coords(time=(\"time\",time_d))\n",
    "\n",
    "        # if precipitation, compute daily value from monthly accumulation and daily fraction\n",
    "        if varnames[varind]=='Precip':\n",
    "            print('computing daily values for Precip...')\n",
    "            # chunk the data\n",
    "            var_acc=data3D_m.chunk(xrchunks)\n",
    "            var_frac=data3D_d.chunk(xrchunks)\n",
    "\n",
    "            # time labels --> months for the groupby in next step\n",
    "            var_acc=var_acc.rename({'time':'month'})\n",
    "            months=np.arange(12)+1\n",
    "            var_acc['month']=months\n",
    "\n",
    "            # lazy parallel calculation\n",
    "            with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "                var_daily=var_frac.groupby('time.month')*var_acc  # times here instead of add\n",
    "            del var_acc, var_frac\n",
    "        # if not precipitation, compute daily value from monthly mean and daily deviation\n",
    "        else:\n",
    "            print('computing daily values for',varnames[varind],'...')\n",
    "            # chunk the data\n",
    "            var_mean=data3D_m.chunk(xrchunks)\n",
    "            var_prime=data3D_d.chunk(xrchunks)\n",
    "\n",
    "            # time labels --> months for the groupby in next step\n",
    "            var_mean=var_mean.rename({'time':'month'})\n",
    "            months=np.arange(12)+1\n",
    "            var_mean['month']=months\n",
    "\n",
    "            # lazy parallel calculation\n",
    "            with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "                var_daily=var_prime.groupby('time.month') + var_mean\n",
    "            del var_mean, var_prime\n",
    "\n",
    "        # change srad units\n",
    "        attrs={}\n",
    "        newvarname=varnames[varind]\n",
    "        if varnames[varind] == 'Srad':\n",
    "            print('changing units to W/m2...')\n",
    "            # attrs=ds[varname].attrs\n",
    "            attrs['units']='W/m2'\n",
    "\n",
    "            # Convert J/m2/day to W/m2\n",
    "            s_per_day=86400\n",
    "            var_daily=var_daily/s_per_day\n",
    "            var_daily.attrs=attrs    \n",
    "\n",
    "        # interpolate winds to 2m and change units\n",
    "        if varnames[varind] == 'Wind-10m':\n",
    "            # interp from 10m to 2m height\n",
    "            print('interpolating wind to 2m...')\n",
    "            z=10\n",
    "            z_adjust=4.87/(np.log(67.8*z-5.42))\n",
    "            var_daily=var_daily*z_adjust\n",
    "\n",
    "            # fix metadata\n",
    "            newvarname='Wind-2m'\n",
    "            attrs={'standard_name':newvarname,'long_name':'2m Wind Speed','units':'m/s'}\n",
    "            var_daily.attrs=attrs   \n",
    "\n",
    "        # drop leap day if there is one and reorder the array dimensions to (y,x,time)\n",
    "        if (yyyy>=1980) & (yyyy<=2024):\n",
    "            dropdate=str(yyyy)+'-02-29'\n",
    "        else: \n",
    "            dropdate='1900-02-29'\n",
    "\n",
    "        with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "            try:\n",
    "                data_out = var_daily.drop_sel(time=dropdate).transpose('y','x','time')\n",
    "                print('dropping date',dropdate)\n",
    "                del dropdate\n",
    "            except:\n",
    "                data_out = var_daily.transpose('y','x','time')    \n",
    "\n",
    "        # limit precision here?\n",
    "        # data_out=np.trunc(data_out*10**output_trunc[varind])/(10**output_trunc[varind]) \n",
    "\n",
    "        print('min/max values of daily data:',data_out.min().compute().data,data_out.max().compute().data)\n",
    "        print('data_out dtype:',data_out.dtype)\n",
    "        \n",
    "        # visual check January\n",
    "        # figure=plt.figure(figsize=(6,4))\n",
    "        # data_out.isel(time=14).plot()\n",
    "        # plt.title(varnames[varind]+' daily data for input to pyaez, 15 Jan '+str(yyyy))    \n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        del var_daily, data3D_m, data3D_d, time_m, time_d\n",
    "        ######################################################################################################\n",
    "        # END STEP 3\n",
    "        ######################################################################################################\n",
    "\n",
    "        ######################################################################################################\n",
    "        # STEP 4: write out npy file\n",
    "        ######################################################################################################\n",
    "        print('################################ STEP 4: WRITING DATA FILES ################################')\n",
    "        out_dir=out_basedir+str(yyyy)+sep+newvarname+sep # directory to write files to\n",
    "\n",
    "        # create dir for writing npy if it doesn't exist\n",
    "        isExist = os.path.exists(out_dir)\n",
    "        if not isExist:\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "        # execute the parallel computation and write files\n",
    "        # the npy stack is chunked along the longitude dimension (axis 1)\n",
    "        print('writing stack to',out_dir+'...')  \n",
    "        da.to_npy_stack(out_dir,data_out.data,axis=1)            \n",
    "        del out_dir, data_out\n",
    "        print('done with',var)\n",
    "        print('####################################################################################')\n",
    "        ######################################################################################################\n",
    "        # END STEP 4\n",
    "        ######################################################################################################   \n",
    "\n",
    "    print('*****************************************')\n",
    "    print('*************** Processing Rhum ***************')    \n",
    "    print('*****************************************')\n",
    "    \n",
    "    ######################################################################################################\n",
    "    # STEP 1: create relative humidity\n",
    "    ######################################################################################################\n",
    "    print('################################ STEP RH1: RELATIVE HUMIDITY CALC ################################')\n",
    "    \n",
    "    # lazy load chunked data arrays and convert vapr units\n",
    "    vapr=da.from_npy_stack(out_basedir+str(yyyy)+'/Vapr/').rechunk(dachunks)*0.1 # hPa-->kPa\n",
    "    tmax=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmax-2m/').rechunk(dachunks)\n",
    "    tmin=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmin-2m/').rechunk(dachunks)\n",
    "\n",
    "    # limit precision\n",
    "    vapr=(np.trunc(vapr*10**3)/(10**3))\n",
    "    tmax=(np.trunc(tmax*10**3)/(10**3))\n",
    "    tmix=(np.trunc(tmin*10**3)/(10**3))\n",
    "    \n",
    "    # lazy parallel calculation and limit precision\n",
    "    print('lazy calc...')\n",
    "    vapr_sat=(0.5*( np.exp((17.27*tmax)/(tmax+237.3)) + np.exp((17.27*tmin)/(tmin+237.3)) )) # kPa\n",
    "    Rhum=(vapr/vapr_sat) # relative humidity in fraction not percent\n",
    "    Rhum=np.trunc(Rhum*10**4)/(10**4) # limit precision\n",
    "\n",
    "    ######################################################################################################\n",
    "    # STEP 2: write out npy file\n",
    "    ######################################################################################################\n",
    "    print('################################ STEP RH2: WRITING DATA FILE ################################')\n",
    "    out_dir=out_basedir+str(yyyy)+sep+'Rhum'+sep # directory to write files to\n",
    "\n",
    "    # create dir for writing npy if it doesn't exist\n",
    "    isExist = os.path.exists(out_dir)\n",
    "    if not isExist:\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    # execute the parallel computation and write files\n",
    "    # the npy stack is chunked along the longitude dimension (axis 1)        \n",
    "    print('computing and writing stack to',out_dir+'...')     \n",
    "    da.to_npy_stack(out_dir,Rhum,axis=1)   \n",
    "    \n",
    "    del out_dir, Rhum, vapr, tmax, tmin, vapr_sat\n",
    "    print('done with Rhum')\n",
    "    print('####################################################################################')        \n",
    "    task_time=(timer()-start_time)/60.\n",
    "    print('DONE',yyyy,'IN',task_time,'MINUTES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27d681-9e70-480c-9240-158d17ca5302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyaezx",
   "language": "python",
   "name": "pyaezx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
