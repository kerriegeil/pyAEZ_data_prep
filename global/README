Scripts in this folder take the global 1980 daily deviation and monthly data 
provided by Gianluca and Gunther and produce the daily input files for pyaez.
____________________________________________________________________________________
____________________________________________________________________________________

To get this repo on HPC Orion:

change directory to where you keep your github repos e.g.
cd /work/hpc/users/username/repos

clone kerrie's repo
git clone https://github.com/kerriegeil/pyAEZ_data_prep.git

the scripts are located at
pyAEZ_data_prep/global/

to pull down updates for this repo:
git pull

if git pull errors because you've made changes to some files try:
git stash
git pull
____________________________________________________________________________________
____________________________________________________________________________________

Scripts should be run in the following order and descriptions follow:
____________________________________________________________________________________
____________________________________________________________________________________

00_pxv_to_dat.f95

Inputs: pxv files containing daily deviations
Outputs: dat files containing daily deviations

Decription: Translate daily deviations from pxv to dat. These files contain 
only grid cells with data present (cells under the mask are not present).

How to Run: at the command line

To test run this and write new dat files without overwriting to the /datasets 
directories you will want to change the output_dir in the fortran program. e.g.
output_dir='/work/hpc/users/username/data/dat/'

And also make sure the output_dir exists before you run the program e.g.
cd /work/hpc/users/username
mkdir data
mkdir data/dat

Then change directory to the location of the fortran programs e.g
cd /work/hpc/users/username/repos/pyAEZ_data_prep/global/

compile the program
gfortran 00_pxv_to_dat.f95 -o 00_pxv_to_dat

execute compiled program
./00_pxv_to_dat

give the script user input
after execution it prompts for a variable name. type one of the suggested variables.
you also get a prompt for year. type 1980. the script needs to be modified to run 
any other year(s).

Optional: To test run this and overwrite the files in the /datasets 
directories you will want to keep the output_dir in the fortran program set to
output_dir='/work/hpc/datasets/un_fao/gaez/global_1980/dailydev/dat/'
and move onto a development node to run the program e.g.
srun --account=xxxx --nodes=1 --tasks-per-node=1 --time=01:00:00 -p development --pty bash -i
gfortran 00_pxv_to_dat.f95 -o 00_pxv_to_dat
./00_pxv_to_dat
type the variable name and year 1980 when it prompts you
when you are finished type exit to move off the development node back to login node
____________________________________________________________________________________

01_dat_to_nc.ipynb

Inputs: dat files containing daily deviations
Outputs: netcdf files containing daily deviations on a global grid

Description: Translate daily deviation dat files to netcdf with appropriate
metadata attached. Also, put data onto a global grid (2160 latitudes x 4320 
longitudes) where cells without data/under the mask are assigned to nan.
Scale factors/unit conversions provided by Gunther are applied during the 
translation process. This script uses a dask local cluster for parallel computing,
so you need to open the jupyter notebook with more nodes and tasks than normal, 
see below.

How to Run: 
Before opening the notebook, you need to build a conda environment in your 
/work/hpc/users/username directory and install a jupyter kernel. Assuming you have
already installed miniconda... create environment and install kernel with:

conda create -c conda-forge --prefix /work/hpc/users/username/envname xarray rioxarray dask numpy pandas netcdf4 glob2 natsort matplotlib ipykernel 

conda activate /work/hpc/users/username/envname

python -m ipykernel install --prefix /work/hpc/users/username/envname --name envname


Then go to HPC orion open on demand webpage to start your jupyter notebook session:
  - account name = xxxx
  - partition = 400p48h
  - QOS = normal
  - hours = 2 
  - number of nodes = 6
  - number of tasks = 120
  - additional slurm parameters = --exclusive
  - additional kernel locations = /work/hpc/users/username/envname/share/jupyter
  
Then run the notebook cells and follow additional instructions inside the notebook.

____________________________________________________________________________________

02_rst_to_nc.ipynb

Inputs: rst files containing monthly data and elevation
Outputs: netcdf files of monthly data, admin mask, elevation on global grid

Description: Translate rst monthly files to netcdf on a global grid (2160 
latitudes x 4320 longitudes) with appropriate metadata attached. Generate a
0/1 administrative mask because the masks provided (ALOSmask_5m.rst) did not 
match each other or where either the monthly or daily deviation files had 
data present. The global mask generated has 2268708 grid cells equal to one,
representing land grid cells with data present. Also, translate elevation to
netcdf. This script does not use parallel computing.

How to Run:
Make sure you have already created the conda environment from above, we use it again.

Then go to HPC orion open on demand webpage to start your jupyter notebook session:
  - account name = xxxx
  - partition = 400p48h
  - QOS = normal
  - hours = 1 (or more if you want)
  - number of nodes = 1
  - number of tasks = 4
  - additional slurm parameters (don't put anything here)
  - additional kernel locations = /work/hpc/users/username/envname/share/jupyter

Then run the notebook cells and follow additional instructions inside the notebook.

____________________________________________________________________________________

03_create_pyaez_daily.ipynb

Inputs: netcdf files containing daily devations and monthly data
Outputs: netcdf files containing daily data

Description: Generate daily data values by adding the daily deviation to the
monthly data. Create two new variables Wind-2m from Wind-10m and Rhum from
Vapr, Tmin-2m, Tmax-2m. This script uses dask single machine parallel computing, so
open the jupyter notebook session with more tasks than normal

How to Run:
Make sure you have already created the conda environment from above, we use it again.

Then go to HPC orion open on demand webpage to start your jupyter notebook session:
  - account name = xxxx
  - partition = 400p48h
  - QOS = normal
  - hours = 1 (or more if you want)
  - number of nodes = 1
  - number of tasks = 20
  - additional slurm parameters = --exclusive
  - additional kernel locations = /work/hpc/users/username/envname/share/jupyter

Then run the notebook cells and follow additional instructions inside the notebook.

____________________________________________________________________________________

04_nc_to_npy_tiff.ipynb

Inputs: netcdf files containing daily data
Outputs: npy files containing daily data, tif files containing mask,elevation

Description: For consistency, translate files of daily data to npy and static files (mask,
elevation) to tif format. These formats are what is currently used in pyaez NB1. This script
uses dask single machine parallel computing, so open the jupyter notebook session with more 
tasks than normal.

How to Run:
Make sure you have already created the conda environment from above, we use it again.

Then go to HPC orion open on demand webpage to start your jupyter notebook session:
  - account name = xxxx
  - partition = 400p48h
  - QOS = normal
  - hours = 1 (or more if you want)
  - number of nodes = 1
  - number of tasks = 20
  - additional slurm parameters = --exclusive
  - additional kernel locations = /work/hpc/users/username/envname/share/jupyter

Then run the notebook cells and follow additional instructions inside the notebook.

____________________________________________________________________________________
____________________________________________________________________________________


Data sources:
Gianluca's Google Drive (address not included for privacy): monthly rst files and daily deviation pxv files
Gunther by FileSender: monthly Srad tif files

Data Storage Locations:
HPC Orion:
/work/hpc/datasets/un_fao/gaez/global_1980/dailydev
/work/hpc/datasets/un_fao/pyaez/global_1980/monthly
/work/hpc/datasets/un_fao/pyaez/static
/work/hpc/datasets/un_fao/pyaez/global_1980/daily

MSU project storage:
/gri/projects/UN_FAO

