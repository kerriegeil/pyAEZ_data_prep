{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a86a5cb9-0a43-42a3-bf83-5d7517641a46",
   "metadata": {},
   "source": [
    "workflow\n",
    "\n",
    "read pxv data to array1D with shape (npts)\n",
    "reshape array1D to array2D, shape (npts,ndays)\n",
    "read in mask2D shape (ny,nx)\n",
    "verify npts at each day of array2D is the same as number of points in mask2D equal to 1\n",
    "reshape mask2D to mask1D of shape (nspace=ny*nx)\n",
    "#save a boolean array of where 1D mask \n",
    "drop all nodata points from mask1D and call it mask_drop, shape (npts)\n",
    "create copy of mask_drop and fill it with the first day of array2D\n",
    "do this last step for every day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58e45d-1604-4e97-8178-69827727264d",
   "metadata": {},
   "source": [
    "note: you cannot process srad or wind on it's own. you must process it with precip. this is because the srad and wind values written to the pxv files span over the fill value used (-9999). so we need to create a mask from a different variable in order to correctly identify which srad/wind grids should be scaled and which are masked grids. I've chosen precip to create the additional mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73521876-ab5a-41f0-8277-33c6a3f26512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from calendar import isleap\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import dask\n",
    "import rioxarray as rio\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time as timer\n",
    "\n",
    "import logging\n",
    "logging.captureWarnings(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3859e2f6-997e-46bc-b03a-32b8af1237ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# things we need to know up front from Gunther\n",
    "###############################################\n",
    "\n",
    "# 1) global grid ny,nx including Antarctia. this is the shape of the ALOS mask\n",
    "ny_global=2160\n",
    "nx_global=4320\n",
    "\n",
    "# 2) global grid ny,nx excluding Antarctica. ny corresponds to IRmax in Gunther's fortran program\n",
    "ny=1800\n",
    "nx=nx_global\n",
    "\n",
    "# 3) data type of what's inside the pxv files (2 byte int = python np.int16, 4 byte float = np.float32)\n",
    "dtype_d=np.int16   # 2 byte integers for daily data\n",
    "dtype_m=np.float32 # 4 byte floats for monthly data\n",
    "\n",
    "# 4) fill value used in the pxv files\n",
    "fillval_d=-9999   # daily\n",
    "fillval_m=-9999.0 # monthly\n",
    "\n",
    "# 5) total number of data points in the pxv files including points set to the fillval (number of lines in file)\n",
    "npts=2295358\n",
    "\n",
    "# 6) total number of data points in the pxv files with valid data values\n",
    "# this is so we can identify which points should be masked and which shouldn't when the data range spans over the fillval\n",
    "# which is a problem for srad\n",
    "npts_valid_d=2287408  # daily files\n",
    "npts_valid_m=2268708  # monthly files\n",
    "\n",
    "# 7) scale factors for putting the data in the pxv files into units in the table below\n",
    "# in alphabetical order by variable name \n",
    "scale_factors=[0.0001,1000.,0.01,0.01,0.01,0.001]\n",
    "# scale_factors=[0.0001,0.01]\n",
    "\n",
    "# Variable\tMonthly data\tDaily deviations/distr.\tScale factor\n",
    "# Precip\t     mm/day\t         %_of_month×100\t       0.0001\n",
    "# Srad\t       J/m2/day\t            kJ/m2/day\t        1000.\n",
    "# Tmax\t         °C\t                 °C×100\t            0.01\n",
    "# Tmin\t         °C\t                 °C×100\t            0.01\n",
    "# Vapr\t         hPa\t                Pa\t            0.01\n",
    "# Wind\t        m/sec\t              mm/sec\t        0.001\n",
    "\n",
    "# 8) how many decimal places the daily output variables should have\n",
    "output_trunc=[4,2,3,3,2,3]\n",
    "# output_trunc=[4,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da41d15-99ee-4198-a47b-0cb251974dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other constants \n",
    "\n",
    "# pxv things\n",
    "pxv_basedir='/work/hpc/datasets/un_fao/gaez_v5/clim/AgERA5/Hist/'\n",
    "dataset='AgERA5'\n",
    "experiment='Hist'\n",
    "pxvsuf='_5m.pxv'\n",
    "connector='_'\n",
    "dailytag='365'\n",
    "sep='/'\n",
    "pxvdirnames=['prec','srad','tmax','tmin','vapr','wind']\n",
    "varnames=['Precip','Srad','Tmax-2m','Tmin-2m','Vapr','Wind-10m']\n",
    "# pxvdirnames=['prec','vapr']\n",
    "# varnames=['Precip','Vapr']\n",
    "\n",
    "# raster things\n",
    "maskfile='/work/hpc/datasets/un_fao/gaez_v5/land/ALOSmask5m_fill.rst'\n",
    "ydimname='y'\n",
    "xdimname='x'\n",
    "\n",
    "# output things\n",
    "out_basedir='/work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/'\n",
    "\n",
    "# years=[str(year) for year in np.arange(1980,1990)]\n",
    "# years=np.arange(1980,1990)\n",
    "years=np.arange(2020,2022)\n",
    "nmonths=12\n",
    "\n",
    "# chunks={'time':-1,'y':-1,'x':216}\n",
    "xrchunks={'time':-1,'y':-1,'x':54}\n",
    "dachunks=(-1,54,-1)\n",
    "# yyyy=2020\n",
    "# varind=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691fb21c-bb6f-4421-9bd2-f61e839427bf",
   "metadata": {},
   "source": [
    "# First, get the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4342e2-7728-4135-bacb-8907a00abd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data points in mask file 2295358 expecting 2295358\n"
     ]
    }
   ],
   "source": [
    "### get the mask from the rst into an array of 2 dims (x,y)\n",
    "### and check the mask has the same number of data grids as the pxv data\n",
    "\n",
    "# open the maskfile but don't include antarctica so mask has shape (y:1800,x:4320)\n",
    "ds=xr.open_dataset(maskfile,engine='rasterio').isel(y=slice(0,ny)).squeeze()\n",
    "del ds.coords['band']\n",
    "\n",
    "# clean up some metadata\n",
    "ds[xdimname]=ds[xdimname].astype(np.float32)\n",
    "ds[ydimname]=ds[ydimname].astype(np.float32)\n",
    "mask2D=ds.band_data\n",
    "\n",
    "# convert to 0 & 1 mask\n",
    "mask2D=xr.where(mask2D>0,1,0).astype('int8')\n",
    "\n",
    "mask1D=mask2D.stack(space=[ydimname,xdimname]) # collapse mask to 1D: 1800*4320 = 7776000 points\n",
    "inds_data=mask1D==1  # keep track of which points are not masked out\n",
    "\n",
    "# check number of data points in the mask\n",
    "npts_mask=int(mask2D.sum().data)\n",
    "print('total data points in mask file', npts_mask,'expecting',npts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14a22692-e478-48cf-8672-58e53a3f0949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################\n",
      "################################ 2020 ################################\n",
      "########################################################################\n",
      "*****************************************\n",
      "*************** Processing Precip ***************\n",
      "*****************************************\n",
      "################################ STEP1: PROCESSING MONTHLY PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values -9999.0 4785.29\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "adding nans...\n",
      "total number of non-missing data points 2268708 expecting 2268708\n",
      "################################ STEP 2: PROCESSING DAILY DEV PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values before scaling: -9999 10000\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "changing dtype...\n",
      "adding nans...\n",
      "scaling...\n",
      "computing additional mask...\n",
      "total number of non-missing data points 2287408 expecting 2287408\n",
      "data min/max values after scaling: 0.0 1.0\n",
      "################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################\n",
      "computing daily values for Precip...\n",
      "dropping date 2020-02-29\n",
      "min/max values of daily data: 0.0 574.68\n",
      "data_out dtype: float32\n",
      "################################ STEP 4: WRITING DATA FILES ################################\n",
      "writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2020/Precip/...\n",
      "done with Precip\n",
      "####################################################################################\n",
      "*****************************************\n",
      "*************** Processing Srad ***************\n",
      "*****************************************\n",
      "################################ STEP1: PROCESSING MONTHLY PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values -9999.0 38049144.0\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "adding nans...\n",
      "total number of non-missing data points 2268708 expecting 2268708\n",
      "################################ STEP 2: PROCESSING DAILY DEV PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values before scaling: -27299 21907\n",
      "apply extra mask? True\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "changing dtype...\n",
      "processing pxv variable with data values that span over the fillval\n",
      "scaling...\n",
      "applying additional mask...\n",
      "total number of non-missing data points 2287408 expecting 2287408\n",
      "data min/max values after scaling: -27299000.0 21907000.0\n",
      "################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################\n",
      "computing daily values for Srad ...\n",
      "changing units to W/m2...\n",
      "dropping date 2020-02-29\n",
      "min/max values of daily data: -0.005787037037037037 452.54453703703706\n",
      "data_out dtype: float64\n",
      "################################ STEP 4: WRITING DATA FILES ################################\n",
      "writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2020/Srad/...\n",
      "done with Srad\n",
      "####################################################################################\n",
      "*****************************************\n",
      "*************** Processing Tmax-2m ***************\n",
      "*****************************************\n",
      "################################ STEP1: PROCESSING MONTHLY PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values -9999.0 48.35532\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "adding nans...\n",
      "total number of non-missing data points 2268708 expecting 2268708\n",
      "################################ STEP 2: PROCESSING DAILY DEV PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values before scaling: -9999 8340\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "changing dtype...\n",
      "adding nans...\n",
      "scaling...\n",
      "data min/max values after scaling: -31.75 83.4\n",
      "################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################\n",
      "computing daily values for Tmax-2m ...\n",
      "dropping date 2020-02-29\n",
      "min/max values of daily data: -55.974438 52.92048\n",
      "data_out dtype: float32\n",
      "################################ STEP 4: WRITING DATA FILES ################################\n",
      "writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2020/Tmax-2m/...\n",
      "done with Tmax-2m\n",
      "####################################################################################\n",
      "*****************************************\n",
      "*************** Processing Tmin-2m ***************\n",
      "*****************************************\n",
      "################################ STEP1: PROCESSING MONTHLY PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values -9999.0 34.170994\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "adding nans...\n",
      "total number of non-missing data points 2268708 expecting 2268708\n",
      "################################ STEP 2: PROCESSING DAILY DEV PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values before scaling: -9999 8340\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "changing dtype...\n",
      "adding nans...\n",
      "scaling...\n",
      "data min/max values after scaling: -29.55 83.4\n",
      "################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################\n",
      "computing daily values for Tmin-2m ...\n",
      "dropping date 2020-02-29\n",
      "min/max values of daily data: -58.938824 40.170444\n",
      "data_out dtype: float32\n",
      "################################ STEP 4: WRITING DATA FILES ################################\n",
      "writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2020/Tmin-2m/...\n",
      "done with Tmin-2m\n",
      "####################################################################################\n",
      "*****************************************\n",
      "*************** Processing Vapr ***************\n",
      "*****************************************\n",
      "################################ STEP1: PROCESSING MONTHLY PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values -9999.0 40.53413\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "adding nans...\n",
      "total number of non-missing data points 2268708 expecting 2268708\n",
      "################################ STEP 2: PROCESSING DAILY DEV PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values before scaling: -9999 2273\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "changing dtype...\n",
      "adding nans...\n",
      "scaling...\n",
      "data min/max values after scaling: -23.63 22.73\n",
      "################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################\n",
      "computing daily values for Vapr ...\n",
      "dropping date 2020-02-29\n",
      "min/max values of daily data: -0.004999399 46.55133\n",
      "data_out dtype: float32\n",
      "################################ STEP 4: WRITING DATA FILES ################################\n",
      "writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2020/Vapr/...\n",
      "done with Vapr\n",
      "####################################################################################\n",
      "*****************************************\n",
      "*************** Processing Wind-10m ***************\n",
      "*****************************************\n",
      "################################ STEP1: PROCESSING MONTHLY PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values -9999.0 14.127642\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "adding nans...\n",
      "total number of non-missing data points 2268708 expecting 2268708\n",
      "################################ STEP 2: PROCESSING DAILY DEV PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values before scaling: -10241 23961\n",
      "apply extra mask? True\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "changing dtype...\n",
      "processing pxv variable with data values that span over the fillval\n",
      "scaling...\n",
      "applying additional mask...\n",
      "total number of non-missing data points 2287408 expecting 2287408\n",
      "data min/max values after scaling: -10.241 23.961\n",
      "################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################\n",
      "computing daily values for Wind-10m ...\n",
      "interpolating wind to 2m...\n",
      "dropping date 2020-02-29\n",
      "min/max values of daily data: -0.00036984697 26.457298\n",
      "data_out dtype: float32\n",
      "################################ STEP 4: WRITING DATA FILES ################################\n",
      "writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2020/Wind-2m/...\n",
      "done with Wind-10m\n",
      "####################################################################################\n",
      "*****************************************\n",
      "*************** Processing Rhum ***************\n",
      "*****************************************\n",
      "################################ STEP RH1: RELATIVE HUMIDITY CALC ################################\n",
      "lazy calc...\n",
      "################################ STEP RH2: WRITING DATA FILE ################################\n",
      "computing and writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2020/Rhum/...\n",
      "done with Rhum\n",
      "####################################################################################\n",
      "DONE 2020 IN 14.538406801223754 MINUTES\n",
      "########################################################################\n",
      "################################ 2021 ################################\n",
      "########################################################################\n",
      "*****************************************\n",
      "*************** Processing Precip ***************\n",
      "*****************************************\n",
      "################################ STEP1: PROCESSING MONTHLY PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values -9999.0 4003.1401\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "adding nans...\n",
      "total number of non-missing data points 2268708 expecting 2268708\n",
      "################################ STEP 2: PROCESSING DAILY DEV PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values before scaling: -9999 10000\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "changing dtype...\n",
      "adding nans...\n",
      "scaling...\n",
      "computing additional mask...\n",
      "total number of non-missing data points 2287408 expecting 2287408\n",
      "data min/max values after scaling: 0.0 1.0\n",
      "################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################\n",
      "computing daily values for Precip...\n",
      "min/max values of daily data: 0.0 439.5764\n",
      "data_out dtype: float32\n",
      "################################ STEP 4: WRITING DATA FILES ################################\n",
      "writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2021/Precip/...\n",
      "done with Precip\n",
      "####################################################################################\n",
      "*****************************************\n",
      "*************** Processing Srad ***************\n",
      "*****************************************\n",
      "################################ STEP1: PROCESSING MONTHLY PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values -9999.0 36119900.0\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "adding nans...\n",
      "total number of non-missing data points 2268708 expecting 2268708\n",
      "################################ STEP 2: PROCESSING DAILY DEV PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values before scaling: -25554 20355\n",
      "apply extra mask? True\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "changing dtype...\n",
      "processing pxv variable with data values that span over the fillval\n",
      "scaling...\n",
      "applying additional mask...\n",
      "total number of non-missing data points 2287408 expecting 2287408\n",
      "data min/max values after scaling: -25554000.0 20355000.0\n",
      "################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################\n",
      "computing daily values for Srad ...\n",
      "changing units to W/m2...\n",
      "min/max values of daily data: -0.005787127459490741 450.80240740740743\n",
      "data_out dtype: float64\n",
      "################################ STEP 4: WRITING DATA FILES ################################\n",
      "writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2021/Srad/...\n",
      "done with Srad\n",
      "####################################################################################\n",
      "*****************************************\n",
      "*************** Processing Tmax-2m ***************\n",
      "*****************************************\n",
      "################################ STEP1: PROCESSING MONTHLY PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values -9999.0 48.394886\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "adding nans...\n",
      "total number of non-missing data points 2268708 expecting 2268708\n",
      "################################ STEP 2: PROCESSING DAILY DEV PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values before scaling: -9999 8783\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "changing dtype...\n",
      "adding nans...\n",
      "scaling...\n",
      "data min/max values after scaling: -27.9 87.83\n",
      "################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################\n",
      "computing daily values for Tmax-2m ...\n",
      "min/max values of daily data: -57.643635 51.982006\n",
      "data_out dtype: float32\n",
      "################################ STEP 4: WRITING DATA FILES ################################\n",
      "writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2021/Tmax-2m/...\n",
      "done with Tmax-2m\n",
      "####################################################################################\n",
      "*****************************************\n",
      "*************** Processing Tmin-2m ***************\n",
      "*****************************************\n",
      "################################ STEP1: PROCESSING MONTHLY PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values -9999.0 34.948658\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "adding nans...\n",
      "total number of non-missing data points 2268708 expecting 2268708\n",
      "################################ STEP 2: PROCESSING DAILY DEV PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values before scaling: -9999 8783\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "changing dtype...\n",
      "adding nans...\n",
      "scaling...\n",
      "data min/max values after scaling: -28.15 87.83\n",
      "################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################\n",
      "computing daily values for Tmin-2m ...\n",
      "min/max values of daily data: -60.53347 45.578594\n",
      "data_out dtype: float32\n",
      "################################ STEP 4: WRITING DATA FILES ################################\n",
      "writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2021/Tmin-2m/...\n",
      "done with Tmin-2m\n",
      "####################################################################################\n",
      "*****************************************\n",
      "*************** Processing Vapr ***************\n",
      "*****************************************\n",
      "################################ STEP1: PROCESSING MONTHLY PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values -9999.0 40.506546\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "adding nans...\n",
      "total number of non-missing data points 2268708 expecting 2268708\n",
      "################################ STEP 2: PROCESSING DAILY DEV PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values before scaling: -9999 2298\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "changing dtype...\n",
      "adding nans...\n",
      "scaling...\n",
      "data min/max values after scaling: -20.67 22.98\n",
      "################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################\n",
      "computing daily values for Vapr ...\n",
      "min/max values of daily data: -0.0049988925 43.416546\n",
      "data_out dtype: float32\n",
      "################################ STEP 4: WRITING DATA FILES ################################\n",
      "writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2021/Vapr/...\n",
      "done with Vapr\n",
      "####################################################################################\n",
      "*****************************************\n",
      "*************** Processing Wind-10m ***************\n",
      "*****************************************\n",
      "################################ STEP1: PROCESSING MONTHLY PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values -9999.0 14.774736\n",
      "apply extra mask? False\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "adding nans...\n",
      "total number of non-missing data points 2268708 expecting 2268708\n",
      "################################ STEP 2: PROCESSING DAILY DEV PXV ################################\n",
      "total data points in pxv file 2295358 expecting 2295358\n",
      "data min/max values before scaling: -12407 20598\n",
      "apply extra mask? True\n",
      "putting 1D data on a 2D grid...\n",
      "concatenating...\n",
      "changing dtype...\n",
      "processing pxv variable with data values that span over the fillval\n",
      "scaling...\n",
      "applying additional mask...\n",
      "total number of non-missing data points 2287408 expecting 2287408\n",
      "data min/max values after scaling: -12.407001 20.598001\n",
      "################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################\n",
      "computing daily values for Wind-10m ...\n",
      "interpolating wind to 2m...\n",
      "min/max values of daily data: -0.0003623573 22.03586\n",
      "data_out dtype: float32\n",
      "################################ STEP 4: WRITING DATA FILES ################################\n",
      "writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2021/Wind-2m/...\n",
      "done with Wind-10m\n",
      "####################################################################################\n",
      "*****************************************\n",
      "*************** Processing Rhum ***************\n",
      "*****************************************\n",
      "################################ STEP RH1: RELATIVE HUMIDITY CALC ################################\n",
      "lazy calc...\n",
      "################################ STEP RH2: WRITING DATA FILE ################################\n",
      "computing and writing stack to /work/hpc/datasets/un_fao/pyaez/inputs/global/daily365_npy/2021/Rhum/...\n",
      "done with Rhum\n",
      "####################################################################################\n",
      "DONE 2021 IN 14.413652312755584 MINUTES\n"
     ]
    }
   ],
   "source": [
    "# function to call with dask delayed\n",
    "def data_to_nd_array(i,inds,arr1D,pxv,arr2D):\n",
    "    arr1D[inds]=pxv.squeeze()  # remove singleton dim (day)\n",
    "    arr2D[:,:]=arr1D.unstack() # put 1D data onto the 2D grid\n",
    "    return arr2D.copy()\n",
    "\n",
    "for yyyy in years:\n",
    "    start_time=timer()\n",
    "    print('########################################################################')\n",
    "    print('################################',yyyy,'################################')\n",
    "    print('########################################################################')\n",
    "\n",
    "    for varind, var in enumerate(varnames):\n",
    "        print('*****************************************')\n",
    "        print('*************** Processing',varnames[varind],'***************')    \n",
    "        print('*****************************************')\n",
    "        ######################################################################################################\n",
    "        #  STEP 1: Translate monthly means from PXV to xarray data structures    \n",
    "        ######################################################################################################\n",
    "        print('################################ STEP1: PROCESSING MONTHLY PXV ################################')\n",
    "        pxvfile_m=pxv_basedir+pxvdirnames[varind]+sep+varnames[varind]+connector+dataset+connector+experiment+connector+str(yyyy)+pxvsuf\n",
    "        filename=pxvfile_m.split(sep)[-1]\n",
    "\n",
    "        # read file to 1D array\n",
    "        # monthly files have more data in them than we need so we subset the read with count=\n",
    "        with open(pxvfile_m,'rb') as f:\n",
    "            array1D_m=np.fromfile(f,dtype=dtype_m,count=nmonths*npts)\n",
    "\n",
    "        # limit precision here?\n",
    "\n",
    "        # reshape the array to (npoints,ndays)\n",
    "        nvals=array1D_m.shape[0]             # total number of data values\n",
    "        npts_flt=nvals/nmonths               # number of grid points in the file, float format\n",
    "        npts_int=int(nvals/nmonths)              # convert to integer\n",
    "\n",
    "        # check that number of grids found in flt and int are equivalent, if not, the file was read incorrectly\n",
    "        assert npts_flt*10==float(npts_int*10), f\"reading pxv file {filename} with incorrect number of days: {ndays}\"\n",
    "        # check that number of grids found in the file is the number expected, if not, coordinate with Gunther\n",
    "        assert npts_int==npts, f\"pxv file {filename} has {npts_int} total data points, expecting {npts} total data points\"\n",
    "        # print('nvalues in pxv file',nvals)\n",
    "        # print('nmonths in pxv file',nmonths)\n",
    "        print('total data points in pxv file',npts_int,'expecting',npts)\n",
    "\n",
    "        # check that number of grids found in pxv file is the same as the number of grids=1 in the mask file\n",
    "        assert npts_int==npts_mask, f\"npts in pxv is {npts}, npts in mask is {npts_mask}\"\n",
    "\n",
    "        array2D_m=array1D_m.reshape(npts,nmonths) # reshape\n",
    "        # print(array2D_m[15,:])\n",
    "\n",
    "        # find out if data value range spans across the fillvalue\n",
    "        # if it does, we'll need to apply an extra mask later\n",
    "        flag_m=True if array2D_m.min() < fillval_m else False\n",
    "\n",
    "        # check data values\n",
    "        # print('shape of numpy data array',array2D_m.shape)\n",
    "        print('data min/max values',array2D_m.min(),array2D_m.max())\n",
    "        print('apply extra mask?',flag_m)\n",
    "\n",
    "        # set up for putting data on full grid\n",
    "        empty1D_m=mask1D.copy().astype(dtype_m)  # placeholder array for 1D space \n",
    "        empty1D_m.rio.write_nodata(fillval_m,inplace=True)\n",
    "        empty1D_m[:]=fillval_m\n",
    "\n",
    "        empty2D_m=mask2D.copy().astype(dtype_m)  # placeholder array for 2D grid \n",
    "        empty2D_m.rio.write_nodata(fillval_m,inplace=True)\n",
    "        empty2D_m[:,:]=fillval_m  \n",
    "\n",
    "        # dask parallel computing \n",
    "        # first convert to pxv data to chunked dask array, 1 day per chunk \n",
    "        # and save to list of delayed dask objects\n",
    "        pxv_delay=da.from_array(array2D_m,chunks=(-1,1)).to_delayed().ravel() \n",
    "\n",
    "        # build a list a computational tasks to be executed later\n",
    "        task_list=[dask.delayed(data_to_nd_array)(imonth,inds_data,empty1D_m.copy(),pxvdata,empty2D_m.copy()) for imonth,pxvdata in enumerate(pxv_delay)] \n",
    "        assert len(task_list)==nmonths, f'{len(task_list)} tasks in list, should be {nmonths}' # double check we've got 1 task per day of data\n",
    "\n",
    "        # execute all computations\n",
    "        print('putting 1D data on a 2D grid...')\n",
    "        result_chunks_m=dask.compute(*task_list)\n",
    "\n",
    "        # concatenate the resulting daily chunks along a new time dimension\n",
    "        print('concatenating...')\n",
    "        data3D_m=xr.concat(result_chunks_m,dim='time')\n",
    "\n",
    "        # replace fillval with nan\n",
    "        print('adding nans...')\n",
    "        data3D_m=xr.where(data3D_m==fillval_m,np.nan,data3D_m)\n",
    "\n",
    "        # check we have the correct number of non-missing data points\n",
    "        data_mask_m=xr.where(np.isnan(data3D_m.data),0,1)  \n",
    "        ngrids_data_m=int(data_mask_m.sum()/data_mask_m.shape[0])\n",
    "        print('total number of non-missing data points',ngrids_data_m,'expecting',npts_valid_m)\n",
    "        assert ngrids_data_m==npts_valid_m, f'data mask creation issue. found {ngrids_data_m} valid data points (non missing), expecting {npts_valid_m}' # double check we've got 1 task per day of data    \n",
    "\n",
    "        # visual check January\n",
    "        # figure=plt.figure(figsize=(6,4))\n",
    "        # data3D_m.isel(time=0).plot()\n",
    "        # plt.title(varnames[varind]+' data from monthly mean pxv, Jan '+str(yyyy))\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        del array1D_m,pxvfile_m,filename,nvals,npts_flt,npts_int,array2D_m,flag_m,empty1D_m,empty2D_m,pxv_delay,task_list,result_chunks_m\n",
    "        ######################################################################################################\n",
    "        # END STEP 1\n",
    "        #####################################################################################################\n",
    "\n",
    "        ######################################################################################################\n",
    "        # STEP 2: Translate daily deviations from PXV to scaled xarray data structures\n",
    "        ######################################################################################################\n",
    "        ### get the data from the pxv into an array of 2 dims (space,time)\n",
    "        print('################################ STEP 2: PROCESSING DAILY DEV PXV ################################')\n",
    "        pxvfile=pxv_basedir+pxvdirnames[varind]+sep+varnames[varind]+dailytag+connector+dataset+connector+experiment+connector+str(yyyy)+pxvsuf\n",
    "        filename=pxvfile.split(sep)[-1]\n",
    "\n",
    "        # read entire file into 1D array\n",
    "        with open(pxvfile,'rb') as f:\n",
    "            array1D_d=np.fromfile(f,dtype=dtype_d)            \n",
    "\n",
    "        # reshape the array to (npoints,ndays)\n",
    "        nvals=array1D_d.shape[0]             # total number of data values\n",
    "        ndays=366 if isleap(yyyy) else 365 # number of days of data at each grid point\n",
    "        npts_flt=nvals/ndays               # number of grid points in the file, float format\n",
    "        npts_int=int(nvals/ndays)              # convert to integer\n",
    "\n",
    "        # check that number of grids found in flt and int are equivalent, if not, the file was read incorrectly\n",
    "        assert npts_flt*10==float(npts_int*10), f\"reading pxv file {filename} with incorrect number of days: {ndays}\"\n",
    "        # check that number of grids found in the file is the number expected, if not, coordinate with Gunther\n",
    "        assert npts_int==npts, f\"pxv file {filename} has {npts_int} total data points, expecting {npts} total data points\"\n",
    "        # print('nvalues in pxv file',nvals)\n",
    "        # print('ndays in pxv file',ndays)\n",
    "        print('total data points in pxv file',npts_int,'expecting',npts)\n",
    "\n",
    "        # check that number of grids found in pxv file is the same as the number of grids=1 in the mask file\n",
    "        assert npts_int==npts_mask, f\"npts in pxv is {npts}, npts in mask is {npts_mask}\"\n",
    "\n",
    "        array2D_d=array1D_d.reshape(npts,ndays) # reshape\n",
    "\n",
    "        # find out if data value range spans across the fillvalue\n",
    "        # if it does, we'll need to apply an extra mask later\n",
    "        flag_d=True if array2D_d.min() < fillval_d else False\n",
    "\n",
    "        # check data values\n",
    "        # print('shape of numpy data array',array2D_d.shape)\n",
    "        print('data min/max values before scaling:',array2D_d.min(),array2D_d.max())\n",
    "        print('apply extra mask?',flag_d)\n",
    "        # array2D[0,:]   \n",
    "\n",
    "        # set up for putting data on full grid\n",
    "        empty1D_d=mask1D.copy().astype(dtype_d)  # placeholder array for 1D space \n",
    "        empty1D_d.rio.write_nodata(fillval_d,inplace=True)\n",
    "        empty1D_d[:]=fillval_d\n",
    "\n",
    "        empty2D_d=mask2D.copy().astype(dtype_d)  # placeholder array for 2D grid \n",
    "        empty2D_d.rio.write_nodata(fillval_d,inplace=True)\n",
    "        empty2D_d[:,:]=fillval_d\n",
    "\n",
    "        # dask parallel computing \n",
    "        # first convert to pxv data to chunked dask array, 1 day per chunk \n",
    "        # and save to list of delayed dask objects\n",
    "        pxv_delay=da.from_array(array2D_d,chunks=(-1,1)).to_delayed().ravel() \n",
    "\n",
    "        # build a list a computational tasks to be executed later\n",
    "        task_list=[dask.delayed(data_to_nd_array)(iday,inds_data,empty1D_d.copy(),pxvdata,empty2D_d.copy()) for iday,pxvdata in enumerate(pxv_delay)] \n",
    "        assert len(task_list)==ndays, f'{len(task_list)} tasks in list, should be {ndays}' # double check we've got 1 task per day of data\n",
    "\n",
    "        # execute all computations\n",
    "        print('putting 1D data on a 2D grid...')\n",
    "        result_chunks_d=dask.compute(*task_list)\n",
    "\n",
    "        # concatenate the resulting daily chunks along a new time dimension\n",
    "        print('concatenating...')\n",
    "        data3D_d=xr.concat(result_chunks_d,dim='time')\n",
    "\n",
    "        # change out the fill value to nans\n",
    "        # this is where we need to apply an extra mask if the valid range of the data includes the -9999 fillval\n",
    "        # which is the case for srad ************ADD OTHERS HERE************************\n",
    "\n",
    "        print('changing dtype...')      \n",
    "        # we need to force float32 if we want to have nans in the output\n",
    "        data3D_d=data3D_d.astype(np.float32)\n",
    "\n",
    "        # if valid data range includes fillval (srad, wind), scale then additional masking\n",
    "        if flag_d:\n",
    "            print('processing pxv variable with data values that span over the fillval')\n",
    "            print('scaling...')\n",
    "            data3D_d=data3D_d*scale_factors[varind]    \n",
    "            print('applying additional mask...')\n",
    "            data3D_d=xr.where(data_mask_d,data3D_d,np.nan)\n",
    "            # verify that the masking worked\n",
    "            valid_arr=np.where(np.isnan(data3D_d.data[14,:,:]),0,1) # pick one day to verify  \n",
    "            nvalid=valid_arr.sum()\n",
    "            print('total number of non-missing data points',nvalid,'expecting',npts_valid_d)\n",
    "            assert nvalid==npts_valid_d, f'data mask application issue. found {nvalid} valid data points (non missing), expecting {npts_valid_d}'    \n",
    "            del valid_arr,nvalid\n",
    "        # if valid data range doesn't include fillval (precip,tmin,tmax,vapr), convert fillval to nan then scale\n",
    "        else:\n",
    "            print('adding nans...')\n",
    "            data3D_d=xr.where(data3D_d==fillval_d,np.nan,data3D_d)\n",
    "            print('scaling...')\n",
    "            data3D_d=data3D_d*scale_factors[varind]\n",
    "\n",
    "            if varnames[varind] =='Precip':\n",
    "                print('computing additional mask...')\n",
    "                data_mask_d=xr.where(np.isnan(data3D_d.data),0,1)  \n",
    "                ngrids_data_d=int(data_mask_d.sum()/data_mask_d.shape[0])\n",
    "                print('total number of non-missing data points',ngrids_data_d,'expecting',npts_valid_d)\n",
    "                assert ngrids_data_d==npts_valid_d, f'data mask creation issue. found {ngrids_data_d} valid data points (non missing), expecting {npts_valid_d}' # double check we've got 1 task per day of data\n",
    "\n",
    "        print('data min/max values after scaling:',data3D_d.min().data,data3D_d.max().data)\n",
    "                \n",
    "        # figure=plt.figure(figsize=(6,4))\n",
    "        # data3D_d.isel(time=14).plot()\n",
    "        # plt.title(varnames[varind]+' scaled data from daily dev pxv, 15 Jan '+str(yyyy))\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        del array1D_d,pxvfile,filename,nvals,npts_flt,npts_int,array2D_d,flag_d,empty1D_d,empty2D_d,pxv_delay,task_list,result_chunks_d\n",
    "        ######################################################################################################\n",
    "        # END STEP 2\n",
    "        ######################################################################################################\n",
    "\n",
    "        ######################################################################################################\n",
    "        # STEP 3: Create daily data from monthly means and daily deviations\n",
    "        ######################################################################################################\n",
    "        print('################################ STEP 3: CREATING DAILY DATA FROM MONTHLY MEAN AND DAILY DEV ################################')\n",
    "        if (yyyy>=1980) & (yyyy<=2024):\n",
    "            time_m=pd.date_range(str(yyyy)+'-01-01',str(yyyy)+'-12-31',freq='MS')\n",
    "            time_d=pd.date_range(str(yyyy)+'-01-01',str(yyyy)+'-12-31',freq='D')\n",
    "        else:\n",
    "            time_m=pd.date_range('1900-01-01','1900-12-31',freq='MS')  \n",
    "            time_d=pd.date_range('1900-01-01','1900-12-31',freq='D') \n",
    "\n",
    "        data3D_m=data3D_m.assign_coords(time=(\"time\",time_m))\n",
    "        data3D_d=data3D_d.assign_coords(time=(\"time\",time_d))\n",
    "\n",
    "        if varnames[varind]=='Precip':\n",
    "            print('computing daily values for Precip...')\n",
    "            var_acc=data3D_m.chunk(xrchunks)\n",
    "            var_frac=data3D_d.chunk(xrchunks)\n",
    "\n",
    "            var_acc=var_acc.rename({'time':'month'})\n",
    "            months=np.arange(12)+1\n",
    "            var_acc['month']=months\n",
    "\n",
    "            with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "                var_daily=var_frac.groupby('time.month')*var_acc  # times here instead of add\n",
    "            del var_acc, var_frac\n",
    "        else:\n",
    "            print('computing daily values for',varnames[varind],'...')\n",
    "            var_mean=data3D_m.chunk(xrchunks)\n",
    "            var_prime=data3D_d.chunk(xrchunks)\n",
    "\n",
    "            var_mean=var_mean.rename({'time':'month'})\n",
    "            months=np.arange(12)+1\n",
    "            var_mean['month']=months\n",
    "\n",
    "            with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "                var_daily=var_prime.groupby('time.month') + var_mean\n",
    "            del var_mean, var_prime\n",
    "\n",
    "        attrs={}\n",
    "        newvarname=varnames[varind]\n",
    "        if varnames[varind] == 'Srad':\n",
    "            print('changing units to W/m2...')\n",
    "            # attrs=ds[varname].attrs\n",
    "            attrs['units']='W/m2'\n",
    "\n",
    "            # Convert J/m2/day to W/m2\n",
    "            s_per_day=86400\n",
    "            var_daily=var_daily/s_per_day\n",
    "            var_daily.attrs=attrs    \n",
    "\n",
    "        if varnames[varind] == 'Wind-10m':\n",
    "            # interp from 10m to 2m height\n",
    "            print('interpolating wind to 2m...')\n",
    "            z=10\n",
    "            z_adjust=4.87/(np.log(67.8*z-5.42))\n",
    "            var_daily=var_daily*z_adjust\n",
    "\n",
    "            # fix metadata\n",
    "            newvarname='Wind-2m'\n",
    "            attrs={'standard_name':newvarname,'long_name':'2m Wind Speed','units':'m/s'}\n",
    "            var_daily.attrs=attrs   \n",
    "\n",
    "        if (yyyy>=1980) & (yyyy<=2024):\n",
    "            dropdate=str(yyyy)+'-02-29'\n",
    "        else: \n",
    "            dropdate='1900-02-29'\n",
    "\n",
    "        with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "            try:\n",
    "                data_out = var_daily.drop_sel(time=dropdate).transpose('y','x','time')\n",
    "                print('dropping date',dropdate)\n",
    "                del dropdate\n",
    "            except:\n",
    "                data_out = var_daily.transpose('y','x','time')    \n",
    "\n",
    "        # limit precision here?\n",
    "        # data_out=np.trunc(data_out*10**output_trunc[varind])/(10**output_trunc[varind]) \n",
    "\n",
    "        print('min/max values of daily data:',data_out.min().compute().data,data_out.max().compute().data)\n",
    "        print('data_out dtype:',data_out.dtype)\n",
    "        # figure=plt.figure(figsize=(6,4))\n",
    "        # data_out.isel(time=14).plot()\n",
    "        # plt.title(varnames[varind]+' daily data for input to pyaez, 15 Jan '+str(yyyy))    \n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "        del var_daily, data3D_m, data3D_d, time_m, time_d\n",
    "        ######################################################################################################\n",
    "        # END STEP 3\n",
    "        ######################################################################################################\n",
    "\n",
    "        ######################################################################################################\n",
    "        # STEP 4: write out npy file\n",
    "        ######################################################################################################\n",
    "        print('################################ STEP 4: WRITING DATA FILES ################################')\n",
    "        out_dir=out_basedir+str(yyyy)+sep+newvarname+sep\n",
    "\n",
    "        # set up dir for writing npy\n",
    "        isExist = os.path.exists(out_dir)\n",
    "        if not isExist:\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "        print('writing stack to',out_dir+'...')  \n",
    "        # print(data_out)\n",
    "        # data_out=data_out.rechunk(dachunks)\n",
    "        da.to_npy_stack(out_dir,data_out.data,axis=1)            \n",
    "        del out_dir, data_out\n",
    "        print('done with',var)\n",
    "        print('####################################################################################')\n",
    "        ######################################################################################################\n",
    "        # END STEP 4\n",
    "        ######################################################################################################   \n",
    "\n",
    "    print('*****************************************')\n",
    "    print('*************** Processing Rhum ***************')    \n",
    "    print('*****************************************')\n",
    "    ######################################################################################################\n",
    "    # STEP 1: create relative humidity\n",
    "    ######################################################################################################\n",
    "    print('################################ STEP RH1: RELATIVE HUMIDITY CALC ################################')\n",
    "    vapr=da.from_npy_stack(out_basedir+str(yyyy)+'/Vapr/').rechunk(dachunks)*0.1 # hPa-->kPa\n",
    "    tmax=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmax-2m/').rechunk(dachunks)\n",
    "    tmin=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmin-2m/').rechunk(dachunks)\n",
    "\n",
    "    vapr=(np.trunc(vapr*10**3)/(10**3))\n",
    "    tmax=(np.trunc(tmax*10**3)/(10**3))\n",
    "    tmix=(np.trunc(tmin*10**3)/(10**3))\n",
    "    \n",
    "    print('lazy calc...')\n",
    "    vapr_sat=(0.5*( np.exp((17.27*tmax)/(tmax+237.3)) + np.exp((17.27*tmin)/(tmin+237.3)) )) # kPa\n",
    "    Rhum=(vapr/vapr_sat) # fraction, not percent\n",
    "    Rhum=np.trunc(Rhum*10**4)/(10**4)\n",
    "\n",
    "    ######################################################################################################\n",
    "    # STEP 2: write out npy file\n",
    "    ######################################################################################################\n",
    "    print('################################ STEP RH2: WRITING DATA FILE ################################')\n",
    "    out_dir=out_basedir+str(yyyy)+sep+'Rhum'+sep\n",
    "\n",
    "    # set up dir for writing npy\n",
    "    isExist = os.path.exists(out_dir)\n",
    "    if not isExist:\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    print('computing and writing stack to',out_dir+'...')     \n",
    "    da.to_npy_stack(out_dir,Rhum,axis=1)   \n",
    "    \n",
    "    del out_dir, Rhum, vapr, tmax, tmin, vapr_sat\n",
    "    print('done with Rhum')\n",
    "    print('####################################################################################')        \n",
    "    task_time=(timer()-start_time)/60.\n",
    "    print('DONE',yyyy,'IN',task_time,'MINUTES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38b5a7-ddc4-4fa6-b6ac-d7a7f042d3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96974e06-0ef3-4c50-84e0-9b0061e07018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9756c2-075f-4e87-b0c5-65fc483b649b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb57cd4-903a-4d1e-844c-f9eda4bb8b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fd53e-94e6-43b5-87f0-17c24e471f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyyy='2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992e457-6d61-4ad8-ab26-c335f3623db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# single chunk with truncation of everything\n",
    "\n",
    "vapr=da.from_npy_stack(out_basedir+str(yyyy)+'/Vapr/')*0.1 # hPa-->kPa\n",
    "tmax=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmax-2m/')\n",
    "tmin=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmin-2m/')\n",
    "\n",
    "vapr=(np.trunc(vapr*10**3)/(10**3))\n",
    "tmax=(np.trunc(tmax*10**3)/(10**3))\n",
    "tmix=(np.trunc(tmin*10**3)/(10**3))\n",
    "\n",
    "vapr_sat=(0.5*( np.exp((17.27*tmax)/(tmax+237.3)) + np.exp((17.27*tmin)/(tmin+237.3)) ))\n",
    "Rhum=(vapr/vapr_sat)\n",
    "Rhum=np.trunc(Rhum*10**4)/(10**4)\n",
    "Rhum=Rhum.compute()\n",
    "\n",
    "print(np.nanmin(Rhum),np.nanmax(Rhum),np.nanmin(Rhum[:,:,14]),np.nanmax(Rhum[:,:,14]))\n",
    "del vapr, tmax, tmin, vapr_sat, Rhum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095327c-6695-4f36-af5d-7436960bacc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 80 chunks with truncation of everything\n",
    "\n",
    "vapr=da.from_npy_stack(out_basedir+str(yyyy)+'/Vapr/').rechunk(dachunks)*0.1 # hPa-->kPa\n",
    "tmax=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmax-2m/').rechunk(dachunks)\n",
    "tmin=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmin-2m/').rechunk(dachunks)\n",
    "\n",
    "vapr=(np.trunc(vapr*10**3)/(10**3))\n",
    "tmax=(np.trunc(tmax*10**3)/(10**3))\n",
    "tmix=(np.trunc(tmin*10**3)/(10**3))\n",
    "\n",
    "vapr_sat=(0.5*( np.exp((17.27*tmax)/(tmax+237.3)) + np.exp((17.27*tmin)/(tmin+237.3)) ))\n",
    "Rhum=(vapr/vapr_sat)\n",
    "Rhum=np.trunc(Rhum*10**4)/(10**4)\n",
    "Rhum=Rhum.compute()\n",
    "\n",
    "print(np.nanmin(Rhum),np.nanmax(Rhum),np.nanmin(Rhum[:,:,14]),np.nanmax(Rhum[:,:,14]))\n",
    "del vapr, tmax, tmin, vapr_sat, Rhum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507888b-e3da-4d9b-b562-0413df7f4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 80 chunks with truncation of everything\n",
    "\n",
    "vapr=da.from_npy_stack(out_basedir+str(yyyy)+'/Vapr/').astype(np.float16)*0.1 # hPa-->kPa\n",
    "tmax=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmax-2m/').astype(np.float16)\n",
    "tmin=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmin-2m/').astype(np.float16)\n",
    "\n",
    "vapr=(np.trunc(vapr*10**3)/(10**3))\n",
    "tmax=(np.trunc(tmax*10**3)/(10**3))\n",
    "tmix=(np.trunc(tmin*10**3)/(10**3))\n",
    "\n",
    "vapr_sat=(0.5*( np.exp((17.27*tmax)/(tmax+237.3)) + np.exp((17.27*tmin)/(tmin+237.3)) ))\n",
    "Rhum=(vapr/vapr_sat)\n",
    "Rhum=np.trunc(Rhum*10**4)/(10**4)\n",
    "Rhum=Rhum.compute()\n",
    "\n",
    "print(np.nanmin(Rhum),np.nanmax(Rhum),np.nanmin(Rhum[:,:,14]),np.nanmax(Rhum[:,:,14]))\n",
    "del vapr, tmax, tmin, vapr_sat, Rhum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9178f00-43d8-4868-8bcd-8de457e20334",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# single chunk with truncation at the end\n",
    "\n",
    "vapr=da.from_npy_stack(out_basedir+str(yyyy)+'/Vapr/')*0.1\n",
    "tmax=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmax-2m/')\n",
    "tmin=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmin-2m/')\n",
    "\n",
    "vapr_sat=(0.5*( np.exp((17.27*tmax)/(tmax+237.3)) + np.exp((17.27*tmin)/(tmin+237.3)) ))\n",
    "Rhum=(vapr/vapr_sat)\n",
    "Rhum=np.trunc(Rhum*10**4)/(10**4)\n",
    "Rhum=Rhum.compute()\n",
    "\n",
    "print(np.nanmin(Rhum),np.nanmax(Rhum),np.nanmin(Rhum[:,:,14]),np.nanmax(Rhum[:,:,14]))\n",
    "del vapr, tmax, tmin, vapr_sat, Rhum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2902e-573f-4781-98db-9a1ed3dec403",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# single chunk type float16 with truncation at the end\n",
    "\n",
    "vapr=(da.from_npy_stack(out_basedir+str(yyyy)+'/Vapr/')*0.1).astype(np.float16)\n",
    "tmax=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmax-2m/').astype(np.float16)\n",
    "tmin=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmin-2m/').astype(np.float16)\n",
    "\n",
    "vapr_sat=(0.5*( np.exp((17.27*tmax)/(tmax+237.3)) + np.exp((17.27*tmin)/(tmin+237.3)) ))\n",
    "Rhum=(vapr/vapr_sat)\n",
    "Rhum=np.trunc(Rhum*10**4)/(10**4)\n",
    "Rhum=Rhum.compute()\n",
    "\n",
    "print(np.nanmin(Rhum),np.nanmax(Rhum),np.nanmin(Rhum[:,:,14]),np.nanmax(Rhum[:,:,14]))\n",
    "del vapr, tmax, tmin, vapr_sat, Rhum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b556e67-5cbd-480e-a995-03dc9dd6e147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72349458-c6bf-4ecb-b7a1-17800fdad3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f521a-a306-489d-a405-f2a3217588f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vapr=da.from_npy_stack(out_basedir+str(yyyy)+'/Vapr/')#.rechunk(dachunks)*0.1 # hPa-->kPa\n",
    "tmax=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmax-2m/')#.rechunk(dachunks)\n",
    "tmin=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmin-2m/')#.rechunk(dachunks)\n",
    "vapr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df5b66-7a3d-4ab8-ad7d-41c077513521",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vapr=(np.trunc(vapr*10**3)/(10**3)).compute()\n",
    "tmax=(np.trunc(tmax*10**3)/(10**3))\n",
    "tmix=(np.trunc(tmin*10**3)/(10**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617a841-e165-4a9b-9f3c-7126564d9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vapr_sat=(0.5*( np.exp((17.27*tmax)/(tmax+237.3)) + np.exp((17.27*tmin)/(tmin+237.3)) )).compute()\n",
    "Rhum=(vapr/vapr_sat)\n",
    "Rhum=np.trunc(Rhum*10**4)/(10**4)\n",
    "# print('min/max:',np.nanmin(data_out).compute(),np.nanmax(data_out).compute())\n",
    "Rhum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78201c7d-b0eb-45d8-bcfa-6c1193c952ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('calling compute on Rhum')\n",
    "Rhum=Rhum.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b2046-ec6f-4133-b033-02be1aa1ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmin(Rhum),np.nanmax(Rhum),np.nanmin(Rhum[:,:,14]),np.nanmax(Rhum[:,:,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6269ef31-f917-45b7-9fc7-6a9ed65034cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure=plt.figure(figsize=(20,5))\n",
    "plt.imshow(Rhum[:,:,14],interpolation='none')#,vmin=-1,vmax=1)\n",
    "plt.colorbar(shrink=0.9)\n",
    "plt.title('kerrie Rhum')    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc0ef7-65e5-490c-922f-40283adc71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "yyyy='2020'\n",
    "print('*****************************************')\n",
    "print('*************** Processing Rhum ***************')    \n",
    "print('*****************************************')\n",
    "######################################################################################################\n",
    "# STEP 1: create relative humidity\n",
    "######################################################################################################\n",
    "print('################################ STEP RH1: RELATIVE HUMIDITY CALC ################################')\n",
    "vapr=da.from_npy_stack(out_basedir+str(yyyy)+'/Vapr/').rechunk(dachunks).persist()\n",
    "tmax=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmax-2m/').rechunk(dachunks).persist()\n",
    "tmin=da.from_npy_stack(out_basedir+str(yyyy)+'/Tmin-2m/').rechunk(dachunks).persist()\n",
    "\n",
    "# print('getting the min/max...')\n",
    "print('lazy calc...')\n",
    "vapr=vapr*0.1 # hPa-->kPa\n",
    "vapr_sat=0.5*( np.exp((17.27*tmax)/(tmax+237.3)) + np.exp((17.27*tmin)/(tmin+237.3)) ) # kPa\n",
    "vapr_sat=np.trunc(vapr_sat*10**2)/(10**2)\n",
    "Rhum=(vapr/vapr_sat)\n",
    "# data_out=(np.trunc(Rhum*10**4)/(10**4))#.compute()\n",
    "# # print('min/max:',np.nanmin(data_out).compute(),np.nanmax(data_out).compute())\n",
    "\n",
    "# ######################################################################################################\n",
    "# # STEP 2: write out npy file\n",
    "# ######################################################################################################\n",
    "# print('################################ STEP RH2: WRITING DATA FILE ################################')\n",
    "# out_dir=out_basedir+str(yyyy)+sep+'Rhum'+sep\n",
    "\n",
    "# # set up dir for writing npy\n",
    "# isExist = os.path.exists(out_dir)\n",
    "# if not isExist:\n",
    "#     os.makedirs(out_dir)\n",
    "\n",
    "# print('computing and writing to',out_dir+'0.npy...')     \n",
    "# da.to_npy_stack(out_dir,data_out,axis=2)            \n",
    "# del out_dir, data_out\n",
    "# print('done with Rhum')\n",
    "# print('####################################################################################')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d5c94-289c-4778-9d59-f63e9e7abcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rhum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd85f598-f0b3-4fa3-a321-a4b935c73ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rhum=Rhum.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64360002-9972-43a5-a6f0-8e7b445d2a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmin(Rhum),np.nanmax(Rhum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd5286-9564-4e02-addb-30f2f69d50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmin(vapr).compute(),np.nanmax(vapr).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d85a25-475f-4c0b-a7c6-e16e37f726d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmin(vapr_sat).compute(),np.nanmax(vapr_sat).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55daf0a-85a6-4e1b-9a92-f6e507e8550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vapr_sat=(np.trunc(vapr_sat*10**2)/(10**2)).compute()\n",
    "vapr_sat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1252683-4648-4c5e-864c-c9ce8d880db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmin(vapr_sat),np.nanmax(vapr_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc8295-faac-455a-b88e-a44fb3d5d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "vapr=vapr.compute()\n",
    "Rhum=(vapr/vapr_sat)\n",
    "np.nanmin(Rhum),np.nanmax(Rhum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e3727-3a6a-4fb6-bc53-bcc3723d8b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258aa6cf-3bb5-4e52-b1c6-96f942629ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhtest=da.from_npy_stack(out_basedir+str(yyyy)+'/Rhum/')[:,:,0]\n",
    "rhtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808e029-03e6-42fc-9f13-b88849a63d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "vapr=da.from_npy_stack(out_basedir+str(yyyy)+'/Vapr/').astype(np.float16).rechunk(dachunks)\n",
    "vapr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea2848-dbb8-4b82-96ad-1bb0489737fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vapr[0,0,:].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f68ac0-da08-42ab-8e0e-383424a47879",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=0\n",
    "figure=plt.figure(figsize=(20,5))\n",
    "idoy=idoys[ind]\n",
    "\n",
    "kerrie_file=kerrie_path+'Rhum/'\n",
    "kdata=da.from_npy_stack(kerrie_file).compute()\n",
    "minmax=(np.nanmin(kdata),np.nanmax(kdata))\n",
    "print('all data minmax:',minmax)\n",
    "kdata=kdata[:,:,idoy]\n",
    "minmax=(np.nanmin(kdata),np.nanmax(kdata))\n",
    "print('single date minmax:',minmax)\n",
    "\n",
    "kdata=np.where(kdata>=0,1,kdata)\n",
    "kdata=np.where(kdata<0,-1,kdata)\n",
    "\n",
    "plt.imshow(kdata,interpolation='none',vmin=-1,vmax=1)\n",
    "plt.colorbar(shrink=0.9)\n",
    "plt.title('kerrie Rhum '+dates[ind]+' positive vs negative')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0db88-75a2-498d-9344-eef6bd1371d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vapr[:,:,14],interpolation='none')\n",
    "plt.colorbar(shrink=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e20f6-86b8-4a82-b397-6ad9e340f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmin(vapr).compute(),np.nanmax(vapr).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c2e75-cba1-4b6d-92d7-04774ce0b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.finfo(np.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb9237f-4911-4231-8759-b7ca69abcf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.finfo(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99f69b9-dae0-430d-bf36-af0b084b9c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.finfo(np.float32).min,np.finfo(np.float32).max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24c138-9b67-4dd5-9294-c4acc9e39234",
   "metadata": {},
   "outputs": [],
   "source": [
    "for yyyy in years:\n",
    "    print(yyyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650dd291-4e3b-4529-bea3-97dddb9a120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for yyyy in years:\n",
    "    for var in varnames:\n",
    "        indir=out_basedir+str(yyyy)+sep+var+sep\n",
    "        print(indir)\n",
    "        try:\n",
    "            data=da.from_npy_stack(indir)\n",
    "            print(indir,data.shape)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            figure=plt.figure(figsize=(20,8))\n",
    "            plt.imshow(data[:,:,275],interpolation='none')\n",
    "            plt.colorbar(shrink=0.7)\n",
    "            plt.show()\n",
    "        except:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51560ef7-89f7-4cc3-9452-0cd8366be529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3ece0-72d3-4410-947f-78c49de0cd4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b284fb-4959-47cc-b2e0-80ec1b904122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make sure this numpy method yields data exactly the same as the fortran method \n",
    "# # monthly\n",
    "# for yyyy in years[0:1]:\n",
    "#     print('########################################################################')\n",
    "#     print('################################',yyyy,'################################')\n",
    "#     print('########################################################################')\n",
    "\n",
    "#     for var,pdir in zip(varnames[:],pxvdirnames[:]):\n",
    "#         print('Processing',var)    \n",
    "\n",
    "#         # Fortran method: get data from dat files\n",
    "#         datfile='/work/hpc/datasets/un_fao/gaez_v5_intermediate/dat/'+var+'_AgERA5_Hist_'+str(yyyy)+'_5m.dat'\n",
    "#         print('dat file:',datfile)\n",
    "#         datstrings=open(datfile).read().splitlines() \n",
    "#         dat=datstrings[1::2] # grab the lines with the data (every other line)\n",
    "#         data_dat=np.loadtxt(dat,dtype='float32')\n",
    "        \n",
    "#         # Numpy method: get data directly from pxv file\n",
    "#         pxvfile=pxv_basedir+pdir+sep+var+connector+dataset+connector+experiment+connector+str(yyyy)+pxvsuf\n",
    "#         print('pxv file:',pxvfile)\n",
    "#         filename=pxvfile.split(sep)[-1]\n",
    "#         nmonths=12            \n",
    "#         with open(pxvfile,'rb') as f:\n",
    "#             array1D=np.fromfile(f,dtype=dtype_m,count=nmonths*npts)\n",
    "#         array2D=array1D.reshape(npts,nmonths) # reshape\n",
    "        \n",
    "#         decs=3\n",
    "#         data_dat=np.trunc(data_dat*10**decs)/(10**decs)\n",
    "#         array2D=np.trunc(array2D*10**decs)/(10**decs)     \n",
    "        \n",
    "#         print('comparing read methods....')\n",
    "#         for p in np.arange(data_dat.shape[0]):\n",
    "#             unique=np.unique(data_dat[p,:]-array2D[p,:])\n",
    "#             assert len(unique)==1,f'not equal at index point {p}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c7ec3-8b99-4266-8f40-955db5cf3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dat[p,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f357e-c0cf-42df-91a4-f1f3e3359723",
   "metadata": {},
   "outputs": [],
   "source": [
    "array2D[p,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4e1ffe-97c6-4ab3-afb8-797c3f459f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37ba3e-284d-4d7d-88f9-d78dd6368519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make sure this numpy method yields data exactly the same as the fortran method \n",
    "# # daily dev\n",
    "# for yyyy in years[0:1]:\n",
    "#     print('########################################################################')\n",
    "#     print('################################',yyyy,'################################')\n",
    "#     print('########################################################################')\n",
    "\n",
    "#     for var,pdir in zip(varnames[2:3],pxvdirnames[2:3]):\n",
    "#         print('Processing',var)    \n",
    "\n",
    "#         # Fortran method: get data from dat files\n",
    "#         datfile='/work/hpc/datasets/un_fao/gaez_v5_intermediate/dat/'+var+'365_AgERA5_Hist_'+str(yyyy)+'_5m.dat'\n",
    "#         print('dat file:',datfile)\n",
    "#         datstrings=open(datfile).read().splitlines() \n",
    "#         dat=datstrings[1::2] # grab the lines with the data (every other line)\n",
    "#         data_dat=np.loadtxt(dat,dtype='int16')\n",
    "        \n",
    "#         # Nunpy method: get data directly from pxv file\n",
    "#         pxvfile=pxv_basedir+pdir+sep+var+dailytag+connector+dataset+connector+experiment+connector+str(yyyy)+pxvsuf\n",
    "#         print('pxv file:',pxvfile)\n",
    "#         filename=pxvfile.split(sep)[-1]\n",
    "#         with open(pxvfile,'rb') as f:\n",
    "#             array1D=np.fromfile(f,dtype=dtype_d)\n",
    "#         ndays=366 if isleap(yyyy) else 365            \n",
    "#         array2D=array1D.reshape(npts,ndays) # reshape\n",
    "\n",
    "   \n",
    "        \n",
    "#         print('comparing read methods....')\n",
    "#         for p in np.arange(data_dat.shape[0]):\n",
    "#             unique=np.unique(data_dat[p,:]-array2D[p,:])\n",
    "#             assert len(unique)==1,f'not equal at index point {p}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7671316a-791a-41d4-9f22-6b10f48d4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7d6f6-0145-4c28-8c42-cf46d8153896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95843ff5-dd31-4f54-bb62-43fcef1d0f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41702033-c91d-4bc5-a479-0457cc46de0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d3d02-cbab-4a1a-b1f9-6eebd0f08d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3894213-6def-4b93-a742-e88e676f6157",
   "metadata": {},
   "source": [
    "# Translate monthly means from PXV to xarray data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38660368-2599-43a5-b547-29708a3314fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pxvfile_m=pxv_basedir+pxvdirnames[varind]+sep+varnames[varind]+connector+dataset+connector+experiment+connector+str(yyyy)+pxvsuf\n",
    "filename=pxvfile_m.split(sep)[-1]\n",
    "\n",
    "nmonths=12\n",
    "\n",
    "# read file to 1D array\n",
    "# monthly files have more data in them than we need so we subset the read with count=\n",
    "with open(pxvfile_m,'rb') as f:\n",
    "    array1D_m=np.fromfile(f,dtype=dtype_m,count=nmonths*npts)\n",
    "\n",
    "# limit precision here?\n",
    "# decs=4\n",
    "# array1D=np.trunc(array1D*10**decs)/(10**decs)\n",
    "\n",
    "# reshape the array to (npoints,ndays)\n",
    "nvals=array1D_m.shape[0]             # total number of data values\n",
    "npts_flt=nvals/nmonths               # number of grid points in the file, float format\n",
    "npts_int=int(nvals/nmonths)              # convert to integer\n",
    "\n",
    "# check that number of grids found in flt and int are equivalent, if not, the file was read incorrectly\n",
    "assert npts_flt*10==float(npts_int*10), f\"reading pxv file {filename} with incorrect number of days: {ndays}\"\n",
    "# check that number of grids found in the file is the number expected, if not, coordinate with Gunther\n",
    "assert npts_int==npts, f\"pxv file {filename} has {npts_int} total data points, expecting {npts} total data points\"\n",
    "print('nvalues in pxv file',nvals)\n",
    "print('nmonths in pxv file',nmonths)\n",
    "print('total data points in pxv file',npts_int,'expecting',npts)\n",
    "\n",
    "# check that number of grids found in pxv file is the same as the number of grids=1 in the mask file\n",
    "npts_mask=int(mask2D.sum().data)\n",
    "print('total data points in mask file', npts_mask,'expecting',npts)\n",
    "assert npts_int==npts_mask, f\"npts in pxv is {npts}, npts in mask is {npts_mask}\"\n",
    "\n",
    "array2D_m=array1D_m.reshape(npts,nmonths) # reshape\n",
    "\n",
    "# find out if data value range spans across the fillvalue\n",
    "# if it does, we'll need to apply an extra mask later\n",
    "flag_m=True if array2D_m.min() < fillval_m else False\n",
    "\n",
    "# check data values\n",
    "print('shape of numpy data array',array2D_m.shape)\n",
    "print('data min/max values',array2D_m.min(),array2D_m.max())\n",
    "print('apply extra mask?',flag_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5b36f-851b-4630-85ff-af40c3aea68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make sure this numpy method reads data exactly the same as the fortran method \n",
    "# temp=open('/work/hpc/datasets/un_fao/gaez_v5_intermediate/dat/Tmax-2m_AgERA5_Hist_2020_5m.dat').read().splitlines() \n",
    "# dat=temp[1::2] # grab the lines with the data (every other line)\n",
    "# data_dat=np.loadtxt(dat,dtype=dtype_m)\n",
    "\n",
    "# decs=3\n",
    "# data_dat=np.trunc(data_dat*10**decs)/(10**decs)\n",
    "# array1D=np.trunc(array1D*10**decs)/(10**decs)\n",
    "\n",
    "# for p in np.arange(data_dat.shape[0]):\n",
    "#     ind1=p*12\n",
    "#     ind2=ind1+12\n",
    "#     unique=np.unique(data_dat[p,:]-array1D[ind1:ind2])\n",
    "#     assert len(unique)==1,f'not equal at index point {p}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535fbcc-491a-4d2d-8a12-ef45a900ce8e",
   "metadata": {},
   "source": [
    "now we need to match each space point of the pxv data to a grid box on the mask\n",
    "\n",
    "first we need to take each month of pxv data from 2295358 points to the full grid of 7776000 space points in 1D\n",
    "\n",
    "then we need to reshape the 7776000 space points to 2D with shape (y:1800,x:4320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcf007b-151a-4934-9cd0-f9d0fdff8dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1D=mask2D.stack(space=[ydimname,xdimname]) # collapse mask to 1D: 1800*4320 = 7776000 points\n",
    "inds_data=mask1D==1  # keep track of which points are not masked out\n",
    "\n",
    "empty1D_m=mask1D.copy().astype(dtype_m)  # placeholder array for 1D space \n",
    "empty1D_m.rio.write_nodata(fillval_m,inplace=True)\n",
    "empty1D_m[:]=fillval_m\n",
    "\n",
    "empty2D_m=mask2D.copy().astype(dtype_m)  # placeholder array for 2D grid \n",
    "empty2D_m.rio.write_nodata(fillval_m,inplace=True)\n",
    "empty2D_m[:,:]=fillval_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12962aba-a4e5-46d9-a739-f9390d8497a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call with dask delayed\n",
    "def data_to_nd_array(i,inds,arr1D,pxv,arr2D):\n",
    "    arr1D[inds]=pxv.squeeze()  # remove singleton dim (day)\n",
    "    arr2D[:,:]=arr1D.unstack() # put 1D data onto the 2D grid\n",
    "    return arr2D.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63851030-001d-4ca0-bbba-14e15eb78eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# dask parallel computing \n",
    "\n",
    "# first convert to pxv data to chunked dask array, 1 day per chunk \n",
    "# and save to list of delayed dask objects\n",
    "pxv_delay=da.from_array(array2D_m,chunks=(-1,1)).to_delayed().ravel() \n",
    "\n",
    "# build a list a computational tasks to be executed later\n",
    "task_list=[dask.delayed(data_to_nd_array)(imonth,inds_data,empty1D_m,pxvdata,empty2D_m) for imonth,pxvdata in enumerate(pxv_delay)] \n",
    "assert len(task_list)==nmonths, f'{len(task_list)} tasks in list, should be {nmonths}' # double check we've got 1 task per day of data\n",
    "\n",
    "# execute all computations\n",
    "print('putting 1D data on a 2D grid...')\n",
    "result_chunks_m=dask.compute(*task_list)\n",
    "\n",
    "# concatenate the resulting daily chunks along a new time dimension\n",
    "print('concatenating...')\n",
    "data3D_m=xr.concat(result_chunks_m,dim='time')\n",
    "\n",
    "# replace fillval with nan\n",
    "print('adding nans...')\n",
    "data3D_m=xr.where(data3D_m==fillval_m,np.nan,data3D_m)\n",
    "\n",
    "# check we have the correct number of non-missing data points\n",
    "data_mask_m=xr.where(np.isnan(data3D_m.data),0,1)  \n",
    "ngrids_data_m=int(data_mask_m.sum()/data_mask_m.shape[0])\n",
    "print('total number of non-missing data points',ngrids_data_m,'expecting',npts_valid_m)\n",
    "assert ngrids_data_m==npts_valid_m, f'data mask creation issue. found {ngrids_data_m} valid data points (non missing), expecting {npts_valid_m}' # double check we've got 1 task per day of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea82f4c-0a40-4923-8503-3eb014d05f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual check January\n",
    "data3D_m.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d9aae1-904e-47bc-a0b8-b3eb930226a1",
   "metadata": {},
   "source": [
    "# Translate daily deviations from PXV to scaled xarray data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3abbb9-b1b3-48c5-82f4-173b66ebf924",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the data from the pxv into an array of 2 dims (space,time)\n",
    "pxvfile=pxv_basedir+pxvdirnames[varind]+sep+varnames[varind]+dailytag+connector+dataset+connector+experiment+connector+str(yyyy)+pxvsuf\n",
    "filename=pxvfile.split(sep)[-1]\n",
    "\n",
    "# read entire file into 1D array\n",
    "with open(pxvfile,'rb') as f:\n",
    "    array1D_d=np.fromfile(f,dtype=dtype_d)\n",
    "    \n",
    "# reshape the array to (npoints,ndays)\n",
    "nvals=array1D_d.shape[0]             # total number of data values\n",
    "ndays=366 if isleap(yyyy) else 365 # number of days of data at each grid point\n",
    "npts_flt=nvals/ndays               # number of grid points in the file, float format\n",
    "npts_int=int(nvals/ndays)              # convert to integer\n",
    "\n",
    "# check that number of grids found in flt and int are equivalent, if not, the file was read incorrectly\n",
    "assert npts_flt*10==float(npts_int*10), f\"reading pxv file {filename} with incorrect number of days: {ndays}\"\n",
    "# check that number of grids found in the file is the number expected, if not, coordinate with Gunther\n",
    "assert npts_int==npts, f\"pxv file {filename} has {npts_int} total data points, expecting {npts} total data points\"\n",
    "print('nvalues in pxv file',nvals)\n",
    "print('ndays in pxv file',ndays)\n",
    "print('total data points in pxv file',npts_int,'expecting',npts)\n",
    "\n",
    "# check that number of grids found in pxv file is the same as the number of grids=1 in the mask file\n",
    "npts_mask=int(mask2D.sum().data)\n",
    "print('total data points in mask file', npts_mask,'expecting',npts)\n",
    "assert npts_int==npts_mask, f\"npts in pxv is {npts}, npts in mask is {npts_mask}\"\n",
    "\n",
    "array2D_d=array1D_d.reshape(npts,ndays) # reshape\n",
    "\n",
    "# find out if data value range spans across the fillvalue\n",
    "# if it does, we'll need to apply an extra mask later\n",
    "flag_d=True if array2D_d.min() < fillval_d else False\n",
    "\n",
    "# check data values\n",
    "print('shape of numpy data array',array2D_d.shape)\n",
    "print('data min/max values',array2D_d.min(),array2D_d.max())\n",
    "print('apply extra mask?',flag_d)\n",
    "# array2D[0,:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92ed9b-d9cf-4a15-994d-fbbea149cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make sure this numpy method reads data exactly the same as the fortran method \n",
    "# temp=open('/work/hpc/datasets/un_fao/gaez_v5_intermediate/dat/Tmax-2m365_AgERA5_Hist_2020_5m.dat').read().splitlines() \n",
    "# dat=temp[1::2] # grab the lines with the data (every other line)\n",
    "# data_dat=np.loadtxt(dat,dtype='int16')\n",
    "# for p in np.arange(array2D.shape[0]):\n",
    "#     unique=np.unique(data_dat[p,:]-array2D[p,:])\n",
    "#     assert len(unique)==1,f'not equal at index point {p}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e84f9c-c2d5-4461-a644-5c686c7d6a12",
   "metadata": {},
   "source": [
    "now we need to match each space point of the pxv data to a grid box on the mask\n",
    "\n",
    "first we need to take each day of pxv data from 2295358 points to the full grid of 7776000 space points in 1D\n",
    "\n",
    "then we need to reshape the 7776000 space points to 2D with shape (y:1800,x:4320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6975a68-f6ac-463b-9ef3-e1368836eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1D=mask2D.stack(space=[ydimname,xdimname]) # collapse mask to 1D: 1800*4320 = 7776000 points\n",
    "inds_data=mask1D==1  # keep track of which points are not masked out\n",
    "\n",
    "empty1D_d=mask1D.copy().astype(dtype_d)  # placeholder array for 1D space \n",
    "empty1D_d.rio.write_nodata(fillval_d,inplace=True)\n",
    "empty1D_d[:]=fillval_d\n",
    "\n",
    "empty2D_d=mask2D.copy().astype(dtype_d)  # placeholder array for 2D grid \n",
    "empty2D_d.rio.write_nodata(fillval_d,inplace=True)\n",
    "empty2D_d[:,:]=fillval_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5714dc2-09da-4630-8359-0aaf0888c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call with dask delayed\n",
    "def data_to_nd_array(i,inds,arr1D,pxv,arr2D):\n",
    "    arr1D[inds]=pxv.squeeze()  # remove singleton dim (day)\n",
    "    arr2D[:,:]=arr1D.unstack() # put 1D data onto the 2D grid\n",
    "    return arr2D.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94908edb-910c-4dc5-af9f-980f9b4c3f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# dask parallel computing \n",
    "\n",
    "# first convert to pxv data to chunked dask array, 1 day per chunk \n",
    "# and save to list of delayed dask objects\n",
    "pxv_delay=da.from_array(array2D_d,chunks=(-1,1)).to_delayed().ravel() \n",
    "\n",
    "# build a list a computational tasks to be executed later\n",
    "task_list=[dask.delayed(data_to_nd_array)(iday,inds_data,empty1D_d,pxvdata,empty2D_d) for iday,pxvdata in enumerate(pxv_delay)] \n",
    "assert len(task_list)==ndays, f'{len(task_list)} tasks in list, should be {ndays}' # double check we've got 1 task per day of data\n",
    "\n",
    "# execute all computations\n",
    "print('putting 1D data on a 2D grid...')\n",
    "result_chunks_d=dask.compute(*task_list)\n",
    "\n",
    "# concatenate the resulting daily chunks along a new time dimension\n",
    "print('concatenating...')\n",
    "data3D_d=xr.concat(result_chunks_d,dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b48fab-e903-4ddd-a98d-3efa68fa6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# change out the fill value to nans\n",
    "# this is where we need to apply an extra mask if the valid range of the data includes the -9999 fillval\n",
    "# which is the case for srad ************ADD OTHERS HERE************************\n",
    "\n",
    "print('changing dtype...')\n",
    "data3D_d=data3D_d.astype(np.float32)\n",
    "\n",
    "# if valid data range includes fillval\n",
    "if flag_d:\n",
    "    print('processing pxv variable with data values that span over the fillval')\n",
    "    print('scaling...')\n",
    "    data3D_d=data3D_d*scale_factors[varind]    \n",
    "    print('applying additional mask...')\n",
    "    data3D_d=xr.where(data_mask,data3D_d,np.nan)\n",
    "    # verify that the masking worked\n",
    "    valid_arr=np.where(np.isnan(data3D_d.data[15,:,:]),0,1) # pick one day to verify  \n",
    "    nvalid=valid_arr.sum()\n",
    "    print('total number of non-missing data points',nvalid,'expecting',npts_valid_d)\n",
    "    assert nvalid==npts_valid_d, f'data mask application issue. found {nvalid} valid data points (non missing), expecting {npts_valid_d}'    \n",
    "\n",
    "# if valid data range doesn't include fillval\n",
    "else:\n",
    "    print('adding nans...')\n",
    "    data3D_d=xr.where(data3D_d==fillval_d,np.nan,data3D_d)\n",
    "    print('scaling...')\n",
    "    data3D_d=data3D_d*scale_factors[varind]\n",
    "\n",
    "    if varnames[varind] =='Precip':\n",
    "        print('computing additional mask...')\n",
    "        data_mask=xr.where(np.isnan(data3D_d.data),0,1)  \n",
    "        ngrids_data_d=int(data_mask.sum()/data_mask.shape[0])\n",
    "        print('total number of non-missing data points',ngrids_data_d,'expecting',npts_valid_d)\n",
    "        assert ngrids_data_d==npts_valid_d, f'data mask creation issue. found {ngrids_data_d} valid data points (non missing), expecting {npts_valid_d}' # double check we've got 1 task per day of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c9cb6-a7de-40aa-8067-a9c34dc95897",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3D_d.isel(time=15).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6044cdca-d316-4ef9-8e5e-7b18d8b24d70",
   "metadata": {},
   "source": [
    "# Create daily data from monthly means and daily deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147c504-6dff-459f-b4bb-e14eab3fed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks={'time':-1,'y':450,'x':2160}\n",
    "# chunks=(-1,450,2160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e2108-77ab-4da8-bb8e-381832c52315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a time dimension for monthly and daily data\n",
    "if (yyyy>=1980) & (yyyy<=2024):\n",
    "    time_m=pd.date_range(str(yyyy)+'-01-01',str(yyyy)+'-12-31',freq='MS')\n",
    "    time_d=pd.date_range(str(yyyy)+'-01-01',str(yyyy)+'-12-31',freq='D')\n",
    "else:\n",
    "    time_m=pd.date_range('1900-01-01','1900-12-31',freq='MS')  \n",
    "    time_d=pd.date_range('1900-01-01','1900-12-31',freq='D')  \n",
    "\n",
    "# time_m,time_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61035e-926b-4410-ac12-b000670b4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign time metadata to monthly and daily data\n",
    "data3D_m=data3D_m.assign_coords(time=(\"time\",time_m))\n",
    "data3D_d=data3D_d.assign_coords(time=(\"time\",time_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff300716-2d82-4c98-847b-b9ce8cd87c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data3D_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e3e482-9e60-4a0f-847e-6bf6f071c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_mean=da.from_array(data3D_m,chunks=chunks)\n",
    "# var_daily=da.from_array(data3D_d.data,chunks=chunks)\n",
    "# var_mean=data3D_m.chunk(chunks)\n",
    "# var_prime=data3D_d.chunk(chunks)\n",
    "# var_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e15bad-2842-461d-9991-6e3b119cc537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_prime=data3D_d\n",
    "# var_mean=data3D_m\n",
    "\n",
    "\n",
    "\n",
    "if varnames[varind]=='Precip':\n",
    "    print('computing daily values for Precip')\n",
    "    var_acc=data3D_m.chunk(chunks)\n",
    "    var_frac=data3D_d.chunk(chunks)\n",
    "    \n",
    "    var_acc=var_acc.rename({'time':'month'})\n",
    "    months=np.arange(12)+1\n",
    "    var_acc['month']=months\n",
    "    \n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        var_daily=var_frac.groupby('time.month')*var_acc  # times here instead of add\n",
    "else:\n",
    "    print('computing daily values for',varnames[varind])\n",
    "    var_mean=data3D_m.chunk(chunks)\n",
    "    var_prime=data3D_d.chunk(chunks)\n",
    "    \n",
    "    var_mean=var_mean.rename({'time':'month'})\n",
    "    months=np.arange(12)+1\n",
    "    var_mean['month']=months\n",
    "        \n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        var_daily=var_prime.groupby('time.month') + var_mean\n",
    "    \n",
    "# var_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2a3de-b034-45ef-b669-ba27cc1f5adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs={}\n",
    "newvarname=varnames[varind]\n",
    "if varnames[varind] == 'Srad':\n",
    "    print('changing units to W/m2')\n",
    "    # attrs=ds[varname].attrs\n",
    "    attrs['units']='W/m2'\n",
    "    \n",
    "    # Convert J/m2/day to W/m2\n",
    "    s_per_day=86400\n",
    "    var_daily=var_daily/s_per_day\n",
    "    var_daily.attrs=attrs    \n",
    "    \n",
    "if varnames[varind] == 'Wind-10m':\n",
    "    # interp from 10m to 2m height\n",
    "    print('interpolating wind to 2m')\n",
    "    z=10\n",
    "    z_adjust=4.87/(np.log(67.8*z-5.42))\n",
    "    var_daily=var_daily*z_adjust\n",
    "    \n",
    "    # fix metadata\n",
    "    newvarname='Wind-2m'\n",
    "    attrs={'standard_name':newvarname,'long_name':'2m Wind Speed','units':'m/s'}\n",
    "    # ds=ds.rename({varname:newvarname})\n",
    "    var_daily.attrs=attrs    \n",
    "    # varname=newvarname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cacc82c-2e5e-4720-a1ac-baccb56da1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2871b-4b52-4c1e-80cf-102bcba95ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if (yyyy>=1980) & (yyyy<=2024):\n",
    "        dropdate=str(yyyy)+'-02-29'\n",
    "    else: \n",
    "        dropdate='1900-02-29'\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        data_out = var_daily.drop_sel(time=dropdate).transpose('y','x','time')#.data\n",
    "    print('dropping date',dropdate)\n",
    "except:\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        data_out = var_daily.drop_sel(time=dropdate).transpose('y','x','time')#.data\n",
    "data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5913a39-5668-4608-9abe-9957a4393025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# data_out=data_out.compute()\n",
    "# data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39b547-23a3-4858-a040-49dc7579c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(data_out[:,:,15],interpolation='none')\n",
    "# plt.colorbar()\n",
    "data_out.isel(time=15).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72460e-929c-4ea2-99cf-c5f83a90ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmin(data_out.data).compute(),np.nanmax(data_out.data).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c9ce76-0f4d-4f75-8426-a82b9a743eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "out_dir=out_basedir+'testing/'+str(yyyy)+sep+varnames[varind]+sep\n",
    "\n",
    "# set up dir for writing npy\n",
    "# out_dir=npy_dir+year+'/'+var_out+'\n",
    "isExist = os.path.exists(out_dir)\n",
    "if not isExist:\n",
    "    os.makedirs(out_dir)\n",
    "            \n",
    "print('writing to',out_dir+'0.npy')     \n",
    "da.to_npy_stack(out_dir,data_out.data,axis=2)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataprep",
   "language": "python",
   "name": "dataprep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
